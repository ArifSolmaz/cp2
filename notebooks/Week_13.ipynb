{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9380be",
   "metadata": {},
   "source": [
    "# Working with External Data\n",
    "\n",
    "*üìö Computer Programming II ¬∑ üë®‚Äçüè´ Dr. Arif Solmaz*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "session_timer_001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "# ‚è±Ô∏è OTURUM ZAMANLAYICI ‚Äî Bu h√ºcreyi ilk √ßalƒ±≈ütƒ±rƒ±n!\n",
    "#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "import time as _time, datetime as _dt\n",
    "from IPython.display import display, HTML as _HTML\n",
    "\n",
    "_SESSION_START = _time.time()\n",
    "_SESSION_START_STR = _dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "_HEARTBEATS = [_time.time()]\n",
    "_CELLS_RUN = [0]\n",
    "IDLE_THRESHOLD = 300\n",
    "\n",
    "def _heartbeat_hook(*args, **kwargs):\n",
    "    _HEARTBEATS.append(_time.time())\n",
    "    _CELLS_RUN[0] += 1\n",
    "\n",
    "def _calc_active_time():\n",
    "    if len(_HEARTBEATS) < 2: return 0\n",
    "    active = 0\n",
    "    for i in range(1, len(_HEARTBEATS)):\n",
    "        gap = _HEARTBEATS[i] - _HEARTBEATS[i-1]\n",
    "        active += gap if gap <= IDLE_THRESHOLD else 30\n",
    "    return int(active)\n",
    "\n",
    "try:\n",
    "    _ip = get_ipython()\n",
    "    _ip.events.register('pre_run_cell', _heartbeat_hook)\n",
    "except: pass\n",
    "\n",
    "display(_HTML(f\"\"\"<div style='background:linear-gradient(135deg,#667eea,#764ba2);padding:14px 20px;border-radius:10px;color:white;font-family:system-ui;margin:4px 0'><b>‚è±Ô∏è Oturum Ba≈üladƒ±</b> ‚Äî {_SESSION_START_STR}<br><span style='font-size:13px;opacity:.85'>H√ºcre aktiviteniz takip ediliyor. Bitince en alttaki Submit h√ºcresini √ßalƒ±≈ütƒ±rƒ±n.</span></div>\"\"\"))\n",
    "print(f'‚úÖ Zamanlayƒ±cƒ± aktif. Idle e≈üiƒüi: {IDLE_THRESHOLD//60} dk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3272a6f",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "- Read and write CSV files for data logging\n",
    "- Work with JSON data for configuration and APIs\n",
    "- Parse and create XML files for data interchange\n",
    "- Make HTTP requests to web APIs\n",
    "- Handle API responses and errors gracefully\n",
    "- Work with SQLite databases for data storage\n",
    "- Validate and clean external data\n",
    "- Build data pipelines for engineering applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6b4862",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Working with CSV Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84f895",
   "metadata": {},
   "source": [
    "CSV (Comma-Separated Values) is one of the most common data formats used in engineering and data science. Python's built-in `csv` module makes it easy to read and write CSV files. CSV files are simple, human-readable, and compatible with spreadsheet software like Excel.\n",
    "\n",
    "| Method | Description | Use Case |\n",
    "| --- | --- | --- |\n",
    "| `csv.reader()` | Read CSV as lists of values | Simple row-by-row processing |\n",
    "| `csv.writer()` | Write lists to CSV format | Creating data logs |\n",
    "| `csv.DictReader()` | Read CSV as dictionaries | Working with named columns |\n",
    "| `csv.DictWriter()` | Write dictionaries to CSV | Structured data output |\n",
    "\n",
    "### Writing CSV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfdd60",
   "metadata": {},
   "source": [
    "**Figure 1.1: Creating CSV Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Create sample sensor data\n",
    "sensor_data = [\n",
    "    ['timestamp', 'sensor_id', 'temperature', 'humidity', 'pressure'],\n",
    "    ['2025-01-15 10:00:00', 'SENSOR_001', 25.5, 65.2, 1013.25],\n",
    "    ['2025-01-15 10:01:00', 'SENSOR_001', 25.7, 64.8, 1013.30],\n",
    "    ['2025-01-15 10:02:00', 'SENSOR_001', 25.6, 65.0, 1013.28],\n",
    "    ['2025-01-15 10:03:00', 'SENSOR_002', 22.3, 70.1, 1012.95],\n",
    "]\n",
    "\n",
    "# Write to a string buffer (simulates file writing)\n",
    "output = StringIO()\n",
    "writer = csv.writer(output)\n",
    "writer.writerows(sensor_data)\n",
    "\n",
    "# Display the CSV content\n",
    "print(\"Generated CSV:\")\n",
    "print(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4522f",
   "metadata": {},
   "source": [
    "### Reading CSV with csv.reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf7461",
   "metadata": {},
   "source": [
    "**Figure 1.2: Reading CSV as Lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d946150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Sample CSV data\n",
    "csv_data = \"\"\"timestamp,sensor_id,temperature,humidity\n",
    "2025-01-15 10:00:00,TEMP_001,25.5,65.2\n",
    "2025-01-15 10:01:00,TEMP_001,25.7,64.8\n",
    "2025-01-15 10:02:00,PRES_001,25.6,65.0\"\"\"\n",
    "\n",
    "# Read CSV data\n",
    "reader = csv.reader(StringIO(csv_data))\n",
    "\n",
    "# Get header\n",
    "header = next(reader)\n",
    "print(f\"Columns: {header}\")\n",
    "print()\n",
    "\n",
    "# Process rows\n",
    "for row in reader:\n",
    "    timestamp, sensor_id, temp, humidity = row\n",
    "    print(f\"{sensor_id}: {temp}¬∞C, {humidity}% at {timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc846596",
   "metadata": {},
   "source": [
    "### Using DictReader for Named Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc323d1",
   "metadata": {},
   "source": [
    "**Figure 1.3: DictReader for Named Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ebb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "csv_data = \"\"\"sensor_id,value,unit,timestamp\n",
    "TEMP_001,25.5,celsius,2025-01-15 10:00\n",
    "TEMP_001,26.0,celsius,2025-01-15 10:01\n",
    "PRES_001,1013.25,hPa,2025-01-15 10:00\n",
    "HUMI_001,65.2,percent,2025-01-15 10:00\"\"\"\n",
    "\n",
    "# DictReader automatically uses first row as keys\n",
    "reader = csv.DictReader(StringIO(csv_data))\n",
    "\n",
    "print(\"Readings by sensor:\")\n",
    "for row in reader:\n",
    "    print(f\"  {row['sensor_id']}: {row['value']} {row['unit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04751749",
   "metadata": {},
   "source": [
    "### Writing with DictWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121f146",
   "metadata": {},
   "source": [
    "**Figure 1.4: DictWriter for Structured Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4054cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "# Sensor readings as dictionaries\n",
    "readings = [\n",
    "    {'sensor_id': 'TEMP_001', 'value': 25.5, 'unit': 'celsius', 'valid': True},\n",
    "    {'sensor_id': 'TEMP_002', 'value': 26.0, 'unit': 'celsius', 'valid': True},\n",
    "    {'sensor_id': 'PRES_001', 'value': 1013.25, 'unit': 'hPa', 'valid': True},\n",
    "]\n",
    "\n",
    "# Define fieldnames (column order)\n",
    "fieldnames = ['sensor_id', 'value', 'unit', 'valid']\n",
    "\n",
    "# Write to CSV\n",
    "output = StringIO()\n",
    "writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "\n",
    "writer.writeheader()  # Write column headers\n",
    "writer.writerows(readings)  # Write all rows\n",
    "\n",
    "print(\"Generated CSV with DictWriter:\")\n",
    "print(output.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3e3da",
   "metadata": {},
   "source": [
    "### Engineering Example: Sensor Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fdcbf",
   "metadata": {},
   "source": [
    "**Figure 1.5: Analyzing CSV Sensor Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "from collections import defaultdict\n",
    "\n",
    "csv_data = \"\"\"sensor_id,temperature\n",
    "TEMP_001,25.5\n",
    "TEMP_001,25.7\n",
    "TEMP_001,25.6\n",
    "TEMP_001,26.0\n",
    "TEMP_002,22.1\n",
    "TEMP_002,22.3\n",
    "TEMP_002,22.0\"\"\"\n",
    "\n",
    "# Group readings by sensor\n",
    "sensor_readings = defaultdict(list)\n",
    "\n",
    "reader = csv.DictReader(StringIO(csv_data))\n",
    "for row in reader:\n",
    "    sensor_id = row['sensor_id']\n",
    "    temp = float(row['temperature'])\n",
    "    sensor_readings[sensor_id].append(temp)\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"Sensor Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for sensor_id, temps in sensor_readings.items():\n",
    "    count = len(temps)\n",
    "    min_t = min(temps)\n",
    "    max_t = max(temps)\n",
    "    avg_t = sum(temps) / count\n",
    "    print(f\"{sensor_id}:\")\n",
    "    print(f\"  Count: {count} readings\")\n",
    "    print(f\"  Range: {min_t:.1f}¬∞C - {max_t:.1f}¬∞C\")\n",
    "    print(f\"  Average: {avg_t:.2f}¬∞C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbba2b1",
   "metadata": {},
   "source": [
    "> üí° **Note:** Always use `newline=''` when opening files for CSV writing on Windows to prevent extra blank lines between rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b59a0e",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Working with JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29971529",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) is widely used for APIs, configuration files, and data exchange. Python's `json` module provides easy serialization and deserialization. JSON maps directly to Python dictionaries and lists.\n",
    "\n",
    "| Function | Input | Output | Use Case |\n",
    "| --- | --- | --- | --- |\n",
    "| `json.dumps()` | Python object | JSON string | Convert to string |\n",
    "| `json.loads()` | JSON string | Python object | Parse string |\n",
    "| `json.dump()` | Python object + file | Write to file | Save to disk |\n",
    "| `json.load()` | File object | Python object | Read from disk |\n",
    "\n",
    "### Python Dict to JSON String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a03897",
   "metadata": {},
   "source": [
    "**Figure 2.1: Converting Python to JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6909db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Python dictionary\n",
    "sensor_data = {\n",
    "    'sensor_id': 'TEMP_001',\n",
    "    'readings': [25.5, 26.0, 25.8, 25.9],\n",
    "    'metadata': {\n",
    "        'location': 'Room A',\n",
    "        'unit': 'celsius',\n",
    "        'calibrated': True\n",
    "    },\n",
    "    'active': True,\n",
    "    'error_count': 0\n",
    "}\n",
    "\n",
    "# Convert to JSON string with pretty formatting\n",
    "json_str = json.dumps(sensor_data, indent=2)\n",
    "print(\"JSON Output:\")\n",
    "print(json_str)\n",
    "\n",
    "# Compact JSON (no whitespace)\n",
    "compact = json.dumps(sensor_data, separators=(',', ':'))\n",
    "print(f\"\\nCompact: {len(compact)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89b0ec",
   "metadata": {},
   "source": [
    "### JSON String to Python Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48666dfd",
   "metadata": {},
   "source": [
    "**Figure 2.2: Parsing JSON to Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON string (could come from API or file)\n",
    "json_string = '''\n",
    "{\n",
    "    \"sensor_id\": \"PRES_001\",\n",
    "    \"value\": 1013.25,\n",
    "    \"unit\": \"hPa\",\n",
    "    \"valid\": true,\n",
    "    \"error\": null,\n",
    "    \"readings\": [1013.2, 1013.3, 1013.25]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Parse JSON to Python\n",
    "data = json.loads(json_string)\n",
    "\n",
    "print(f\"Type: {type(data)}\")\n",
    "print(f\"Sensor: {data['sensor_id']}\")\n",
    "print(f\"Value: {data['value']} {data['unit']}\")\n",
    "print(f\"Valid: {data['valid']}\")\n",
    "print(f\"Readings: {data['readings']}\")\n",
    "\n",
    "# JSON null becomes Python None\n",
    "print(f\"Error: {data['error']} (type: {type(data['error']).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881bd41e",
   "metadata": {},
   "source": [
    "### JSON Type Mapping\n",
    "\n",
    "| JSON Type | Python Type | Example |\n",
    "| --- | --- | --- |\n",
    "| object | dict | `{\"key\": \"value\"}` |\n",
    "| array | list | `[1, 2, 3]` |\n",
    "| string | str | `\"hello\"` |\n",
    "| number (int) | int | `42` |\n",
    "| number (real) | float | `3.14` |\n",
    "| true/false | True/False | `true` |\n",
    "| null | None | `null` |\n",
    "\n",
    "### Configuration Manager with JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f10d3",
   "metadata": {},
   "source": [
    "**Figure 2.3: JSON Configuration Manager**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd964b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"Manage application configuration with JSON.\"\"\"\n",
    "    \n",
    "    def __init__(self, defaults=None):\n",
    "        self.config = defaults or {}\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get a configuration value.\"\"\"\n",
    "        keys = key.split('.')\n",
    "        value = self.config\n",
    "        for k in keys:\n",
    "            if isinstance(value, dict) and k in value:\n",
    "                value = value[k]\n",
    "            else:\n",
    "                return default\n",
    "        return value\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        \"\"\"Set a configuration value.\"\"\"\n",
    "        self.config[key] = value\n",
    "    \n",
    "    def to_json(self):\n",
    "        \"\"\"Export configuration as JSON.\"\"\"\n",
    "        return json.dumps(self.config, indent=2)\n",
    "    \n",
    "    def from_json(self, json_str):\n",
    "        \"\"\"Import configuration from JSON.\"\"\"\n",
    "        self.config = json.loads(json_str)\n",
    "\n",
    "# Create config with defaults\n",
    "config = ConfigManager({\n",
    "    'sensor': {\n",
    "        'sampling_rate': 1.0,\n",
    "        'timeout': 30\n",
    "    },\n",
    "    'logging': {\n",
    "        'level': 'INFO',\n",
    "        'file': 'sensor.log'\n",
    "    }\n",
    "})\n",
    "\n",
    "print(f\"Sampling rate: {config.get('sensor.sampling_rate')}\")\n",
    "print(f\"Log level: {config.get('logging.level')}\")\n",
    "print(f\"\\nFull config:\\n{config.to_json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bebfc",
   "metadata": {},
   "source": [
    "### Handling Custom Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c954fc",
   "metadata": {},
   "source": [
    "**Figure 2.4: Custom JSON Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3415d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class SensorReading:\n",
    "    def __init__(self, sensor_id, value, timestamp=None):\n",
    "        self.sensor_id = sensor_id\n",
    "        self.value = value\n",
    "        self.timestamp = timestamp or datetime.now()\n",
    "\n",
    "# Custom encoder for complex objects\n",
    "class SensorEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, SensorReading):\n",
    "            return {\n",
    "                '__type__': 'SensorReading',\n",
    "                'sensor_id': obj.sensor_id,\n",
    "                'value': obj.value,\n",
    "                'timestamp': obj.timestamp.isoformat()\n",
    "            }\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        return super().default(obj)\n",
    "\n",
    "# Create readings\n",
    "readings = [\n",
    "    SensorReading('TEMP_001', 25.5),\n",
    "    SensorReading('PRES_001', 1013.25)\n",
    "]\n",
    "\n",
    "# Encode with custom encoder\n",
    "json_output = json.dumps(readings, cls=SensorEncoder, indent=2)\n",
    "print(\"Encoded sensor readings:\")\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7554e",
   "metadata": {},
   "source": [
    "> üí° **Note:** Remember: `dumps`/`loads` work with **s**trings, while `dump`/`load` work with files directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecf131",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Working with XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fdbdf",
   "metadata": {},
   "source": [
    "XML (eXtensible Markup Language) is used in many engineering applications, especially for configuration files, data interchange between systems, and industry standards like OPC-UA. Python's `xml.etree.ElementTree` module provides a simple API for parsing and creating XML.\n",
    "\n",
    "### Creating XML Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29408f80",
   "metadata": {},
   "source": [
    "**Figure 3.1: Building XML Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682748ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Create root element\n",
    "root = ET.Element('sensors')\n",
    "root.set('version', '1.0')\n",
    "root.set('timestamp', '2025-01-15T10:00:00')\n",
    "\n",
    "# Add sensor elements\n",
    "sensor1 = ET.SubElement(root, 'sensor')\n",
    "sensor1.set('id', 'TEMP_001')\n",
    "sensor1.set('type', 'temperature')\n",
    "\n",
    "value1 = ET.SubElement(sensor1, 'value')\n",
    "value1.text = '25.5'\n",
    "\n",
    "unit1 = ET.SubElement(sensor1, 'unit')\n",
    "unit1.text = 'celsius'\n",
    "\n",
    "# Add another sensor\n",
    "sensor2 = ET.SubElement(root, 'sensor')\n",
    "sensor2.set('id', 'PRES_001')\n",
    "sensor2.set('type', 'pressure')\n",
    "\n",
    "value2 = ET.SubElement(sensor2, 'value')\n",
    "value2.text = '1013.25'\n",
    "\n",
    "unit2 = ET.SubElement(sensor2, 'unit')\n",
    "unit2.text = 'hPa'\n",
    "\n",
    "# Convert to string\n",
    "xml_str = ET.tostring(root, encoding='unicode')\n",
    "print(\"Generated XML:\")\n",
    "print(xml_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345daa83",
   "metadata": {},
   "source": [
    "### Parsing XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67dd3",
   "metadata": {},
   "source": [
    "**Figure 3.2: Parsing XML Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# XML data to parse\n",
    "xml_string = '''\n",
    "<readings timestamp=\"2025-01-15T10:00:00\">\n",
    "    <reading sensor=\"TEMP_001\">\n",
    "        <value>25.5</value>\n",
    "        <unit>celsius</unit>\n",
    "    </reading>\n",
    "    <reading sensor=\"TEMP_001\">\n",
    "        <value>25.7</value>\n",
    "        <unit>celsius</unit>\n",
    "    </reading>\n",
    "    <reading sensor=\"PRES_001\">\n",
    "        <value>1013.25</value>\n",
    "        <unit>hPa</unit>\n",
    "    </reading>\n",
    "</readings>\n",
    "'''\n",
    "\n",
    "# Parse XML\n",
    "root = ET.fromstring(xml_string)\n",
    "\n",
    "# Get root attributes\n",
    "print(f\"Timestamp: {root.get('timestamp')}\")\n",
    "print()\n",
    "\n",
    "# Find all readings\n",
    "print(\"Parsed readings:\")\n",
    "for reading in root.findall('reading'):\n",
    "    sensor = reading.get('sensor')\n",
    "    value = float(reading.find('value').text)\n",
    "    unit = reading.find('unit').text\n",
    "    print(f\"  {sensor}: {value} {unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae0d96",
   "metadata": {},
   "source": [
    "### XML to Dictionary Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea993e1",
   "metadata": {},
   "source": [
    "**Figure 3.3: Converting XML to Dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "def xml_to_dict(element):\n",
    "    \"\"\"Convert XML element to dictionary recursively.\"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Add attributes with @ prefix\n",
    "    for key, value in element.attrib.items():\n",
    "        result[f'@{key}'] = value\n",
    "    \n",
    "    # Process child elements\n",
    "    for child in element:\n",
    "        child_data = xml_to_dict(child)\n",
    "        \n",
    "        # Handle multiple children with same tag\n",
    "        if child.tag in result:\n",
    "            if not isinstance(result[child.tag], list):\n",
    "                result[child.tag] = [result[child.tag]]\n",
    "            result[child.tag].append(child_data)\n",
    "        else:\n",
    "            result[child.tag] = child_data\n",
    "    \n",
    "    # If no children, use text content\n",
    "    if not result and element.text:\n",
    "        return element.text.strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "xml_string = '''\n",
    "<sensor id=\"TEMP_001\" type=\"temperature\">\n",
    "    <value>25.5</value>\n",
    "    <unit>celsius</unit>\n",
    "    <config>\n",
    "        <min>-40</min>\n",
    "        <max>85</max>\n",
    "    </config>\n",
    "</sensor>\n",
    "'''\n",
    "\n",
    "root = ET.fromstring(xml_string)\n",
    "sensor_dict = xml_to_dict(root)\n",
    "\n",
    "print(\"XML converted to dictionary:\")\n",
    "print(json.dumps(sensor_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05c5fc",
   "metadata": {},
   "source": [
    "### XPath Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3803b",
   "metadata": {},
   "source": [
    "**Figure 3.4: Using XPath for Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "xml_data = '''\n",
    "<system>\n",
    "    <sensors>\n",
    "        <sensor id=\"TEMP_001\" type=\"temperature\">\n",
    "            <value>25.5</value>\n",
    "        </sensor>\n",
    "        <sensor id=\"TEMP_002\" type=\"temperature\">\n",
    "            <value>26.0</value>\n",
    "        </sensor>\n",
    "        <sensor id=\"PRES_001\" type=\"pressure\">\n",
    "            <value>1013.25</value>\n",
    "        </sensor>\n",
    "    </sensors>\n",
    "    <config>\n",
    "        <interval>5</interval>\n",
    "    </config>\n",
    "</system>\n",
    "'''\n",
    "\n",
    "root = ET.fromstring(xml_data)\n",
    "\n",
    "# Find all sensors\n",
    "print(\"All sensors:\")\n",
    "for sensor in root.findall('.//sensor'):\n",
    "    print(f\"  {sensor.get('id')}: {sensor.get('type')}\")\n",
    "\n",
    "# Find temperature sensors only\n",
    "print(\"\\nTemperature sensors:\")\n",
    "for sensor in root.findall(\".//sensor[@type='temperature']\"):\n",
    "    sid = sensor.get('id')\n",
    "    value = sensor.find('value').text\n",
    "    print(f\"  {sid}: {value}¬∞C\")\n",
    "\n",
    "# Find specific element\n",
    "interval = root.find('.//config/interval')\n",
    "print(f\"\\nInterval: {interval.text}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a5a70",
   "metadata": {},
   "source": [
    "> üí° **Note:** Use XPath expressions like `.//element` to find elements anywhere in the tree, and `[@attr='value']` to filter by attributes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68835433",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Making HTTP Requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2c530",
   "metadata": {},
   "source": [
    "HTTP requests allow your Python programs to communicate with web APIs, fetch data from servers, and interact with web services. While the `requests` library is popular, Python's built-in `urllib` can also handle HTTP operations.\n",
    "\n",
    "| Method | Description | Engineering Use Case |\n",
    "| --- | --- | --- |\n",
    "| `GET` | Retrieve data | Fetch sensor readings from API |\n",
    "| `POST` | Send/create data | Submit new sensor reading |\n",
    "| `PUT` | Update existing data | Update sensor configuration |\n",
    "| `DELETE` | Remove data | Remove old sensor from system |\n",
    "| `PATCH` | Partial update | Update specific sensor field |\n",
    "\n",
    "### Simulated REST API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd80b2",
   "metadata": {},
   "source": [
    "**Figure 4.1: REST API Client Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated REST API client (for demonstration)\n",
    "import json\n",
    "\n",
    "class MockAPIClient:\n",
    "    \"\"\"Simulated REST API client for sensor data.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self._data = {\n",
    "            'sensors': [\n",
    "                {'id': 'TEMP_001', 'value': 25.5, 'unit': 'celsius'},\n",
    "                {'id': 'PRES_001', 'value': 1013.25, 'unit': 'hPa'},\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def get(self, endpoint):\n",
    "        \"\"\"Simulate GET request.\"\"\"\n",
    "        print(f\"GET {self.base_url}{endpoint}\")\n",
    "        \n",
    "        if endpoint == '/sensors':\n",
    "            return {'status': 200, 'data': self._data['sensors']}\n",
    "        elif endpoint.startswith('/sensors/'):\n",
    "            sensor_id = endpoint.split('/')[-1]\n",
    "            for s in self._data['sensors']:\n",
    "                if s['id'] == sensor_id:\n",
    "                    return {'status': 200, 'data': s}\n",
    "            return {'status': 404, 'error': 'Sensor not found'}\n",
    "        return {'status': 404, 'error': 'Unknown endpoint'}\n",
    "    \n",
    "    def post(self, endpoint, data):\n",
    "        \"\"\"Simulate POST request.\"\"\"\n",
    "        print(f\"POST {self.base_url}{endpoint}\")\n",
    "        print(f\"  Body: {json.dumps(data)}\")\n",
    "        \n",
    "        if endpoint == '/sensors':\n",
    "            self._data['sensors'].append(data)\n",
    "            return {'status': 201, 'data': data}\n",
    "        return {'status': 400, 'error': 'Bad request'}\n",
    "\n",
    "# Use the API client\n",
    "api = MockAPIClient('http://api.sensors.io')\n",
    "\n",
    "# GET all sensors\n",
    "print(\"--- Fetching all sensors ---\")\n",
    "response = api.get('/sensors')\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# GET specific sensor\n",
    "print(\"--- Fetching TEMP_001 ---\")\n",
    "response = api.get('/sensors/TEMP_001')\n",
    "print(f\"Response: {response}\\n\")\n",
    "\n",
    "# POST new sensor\n",
    "print(\"--- Adding new sensor ---\")\n",
    "new_sensor = {'id': 'HUMI_001', 'value': 65.0, 'unit': 'percent'}\n",
    "response = api.post('/sensors', new_sensor)\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459a0b4",
   "metadata": {},
   "source": [
    "### API Response Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc845f20",
   "metadata": {},
   "source": [
    "**Figure 4.2: Handling API Responses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIResponse:\n",
    "    \"\"\"Wrapper for API responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, status_code, data=None, error=None):\n",
    "        self.status_code = status_code\n",
    "        self.data = data\n",
    "        self.error = error\n",
    "    \n",
    "    @property\n",
    "    def ok(self):\n",
    "        \"\"\"Check if request was successful.\"\"\"\n",
    "        return 200 <= self.status_code < 300\n",
    "    \n",
    "    def json(self):\n",
    "        \"\"\"Return data as JSON.\"\"\"\n",
    "        return self.data\n",
    "    \n",
    "    def raise_for_status(self):\n",
    "        \"\"\"Raise exception if request failed.\"\"\"\n",
    "        if not self.ok:\n",
    "            raise Exception(f\"HTTP {self.status_code}: {self.error}\")\n",
    "\n",
    "def fetch_sensor_data(sensor_id):\n",
    "    \"\"\"Fetch sensor data with error handling.\"\"\"\n",
    "    # Simulate API responses\n",
    "    responses = {\n",
    "        'TEMP_001': APIResponse(200, {'id': 'TEMP_001', 'value': 25.5}),\n",
    "        'TEMP_002': APIResponse(200, {'id': 'TEMP_002', 'value': 26.0}),\n",
    "        'UNKNOWN': APIResponse(404, error='Sensor not found'),\n",
    "    }\n",
    "    \n",
    "    return responses.get(sensor_id, APIResponse(404, error='Not found'))\n",
    "\n",
    "# Test with different sensors\n",
    "test_ids = ['TEMP_001', 'TEMP_002', 'UNKNOWN']\n",
    "\n",
    "for sensor_id in test_ids:\n",
    "    print(f\"Fetching {sensor_id}...\")\n",
    "    response = fetch_sensor_data(sensor_id)\n",
    "    \n",
    "    if response.ok:\n",
    "        data = response.json()\n",
    "        print(f\"  Success: {data}\")\n",
    "    else:\n",
    "        print(f\"  Error ({response.status_code}): {response.error}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd400b",
   "metadata": {},
   "source": [
    "### Retry Logic Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9abbc2",
   "metadata": {},
   "source": [
    "**Figure 4.3: Implementing Retry Logic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def retry_request(func, max_retries=3, delay=1.0):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "    last_exception = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = func()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            wait_time = delay * (2 ** attempt)  # Exponential backoff\n",
    "            print(f\"  Attempt {attempt + 1} failed: {e}\")\n",
    "            print(f\"  Retrying in {wait_time}s...\")\n",
    "            time.sleep(0.1)  # Short sleep for demo\n",
    "    \n",
    "    raise last_exception\n",
    "\n",
    "# Simulate unreliable API\n",
    "call_count = 0\n",
    "\n",
    "def unreliable_api_call():\n",
    "    \"\"\"Simulates an API that fails sometimes.\"\"\"\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    \n",
    "    # Succeed on 3rd attempt\n",
    "    if call_count < 3:\n",
    "        raise ConnectionError(\"Server temporarily unavailable\")\n",
    "    \n",
    "    return {'status': 'success', 'data': {'value': 25.5}}\n",
    "\n",
    "# Test retry logic\n",
    "print(\"Testing retry logic:\")\n",
    "try:\n",
    "    result = retry_request(unreliable_api_call)\n",
    "    print(f\"Final result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"All retries failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b0bb8",
   "metadata": {},
   "source": [
    "> üí° **Note:** Always implement timeout and retry logic for production API clients to handle network failures gracefully!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e1826c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Working with SQLite Databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df5962e",
   "metadata": {},
   "source": [
    "SQLite is a lightweight, serverless database included with Python. It's perfect for embedded applications, local data storage, and engineering data logging. The entire database is stored in a single file.\n",
    "\n",
    "### Database Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b62216",
   "metadata": {},
   "source": [
    "**Figure 5.1: Simulated Database Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83212d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated SQLite operations (actual sqlite3 not available in browser)\n",
    "class MockDatabase:\n",
    "    \"\"\"Simulated database for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tables = {}\n",
    "        print(f\"Connected to database: {name}\")\n",
    "    \n",
    "    def create_table(self, name, columns):\n",
    "        \"\"\"Create a table with specified columns.\"\"\"\n",
    "        self.tables[name] = {'columns': columns, 'rows': []}\n",
    "        print(f\"Created table '{name}' with columns: {columns}\")\n",
    "    \n",
    "    def insert(self, table, values):\n",
    "        \"\"\"Insert a row into a table.\"\"\"\n",
    "        if table in self.tables:\n",
    "            row_id = len(self.tables[table]['rows']) + 1\n",
    "            self.tables[table]['rows'].append((row_id, *values))\n",
    "            return row_id\n",
    "        raise ValueError(f\"Table '{table}' not found\")\n",
    "    \n",
    "    def select_all(self, table):\n",
    "        \"\"\"Get all rows from a table.\"\"\"\n",
    "        if table in self.tables:\n",
    "            return self.tables[table]['rows']\n",
    "        return []\n",
    "\n",
    "# Create database and table\n",
    "db = MockDatabase('sensors.db')\n",
    "db.create_table('readings', ['id', 'sensor_id', 'value', 'unit', 'timestamp'])\n",
    "\n",
    "# Insert data\n",
    "print(\"\\nInserting readings...\")\n",
    "readings = [\n",
    "    ('TEMP_001', 25.5, 'celsius', '2025-01-15 10:00'),\n",
    "    ('TEMP_001', 25.7, 'celsius', '2025-01-15 10:01'),\n",
    "    ('PRES_001', 1013.25, 'hPa', '2025-01-15 10:00'),\n",
    "]\n",
    "\n",
    "for r in readings:\n",
    "    row_id = db.insert('readings', r)\n",
    "    print(f\"  Inserted row {row_id}: {r[0]} = {r[1]}\")\n",
    "\n",
    "# Query data\n",
    "print(\"\\nAll readings:\")\n",
    "for row in db.select_all('readings'):\n",
    "    print(f\"  {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec41f02",
   "metadata": {},
   "source": [
    "### SQL Query Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97d21a",
   "metadata": {},
   "source": [
    "**Figure 5.2: Common SQL Patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common SQL patterns for sensor data applications\n",
    "sql_patterns = {\n",
    "    'Create Table': '''\n",
    "CREATE TABLE IF NOT EXISTS readings (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    sensor_id TEXT NOT NULL,\n",
    "    value REAL NOT NULL,\n",
    "    unit TEXT NOT NULL,\n",
    "    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    ")''',\n",
    "    \n",
    "    'Insert (Parameterized)': '''\n",
    "INSERT INTO readings (sensor_id, value, unit) \n",
    "VALUES (?, ?, ?)''',\n",
    "    \n",
    "    'Select All': '''\n",
    "SELECT * FROM readings \n",
    "ORDER BY timestamp DESC''',\n",
    "    \n",
    "    'Select by Sensor': '''\n",
    "SELECT * FROM readings \n",
    "WHERE sensor_id = ? \n",
    "ORDER BY timestamp DESC''',\n",
    "    \n",
    "    'Aggregate Statistics': '''\n",
    "SELECT \n",
    "    sensor_id,\n",
    "    COUNT(*) as reading_count,\n",
    "    AVG(value) as avg_value,\n",
    "    MIN(value) as min_value,\n",
    "    MAX(value) as max_value\n",
    "FROM readings\n",
    "GROUP BY sensor_id''',\n",
    "    \n",
    "    'Time Range Query': '''\n",
    "SELECT * FROM readings\n",
    "WHERE timestamp BETWEEN ? AND ?\n",
    "ORDER BY timestamp'''\n",
    "}\n",
    "\n",
    "print(\"Common SQL Patterns for Sensor Data:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, sql in sql_patterns.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(sql.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac175e",
   "metadata": {},
   "source": [
    "### Database Wrapper Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80243535",
   "metadata": {},
   "source": [
    "**Figure 5.3: Sensor Database Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDatabase:\n",
    "    \"\"\"Database wrapper for sensor readings.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.readings = []\n",
    "        self._id_counter = 0\n",
    "    \n",
    "    def add_reading(self, sensor_id, value, unit):\n",
    "        \"\"\"Add a new reading.\"\"\"\n",
    "        self._id_counter += 1\n",
    "        reading = {\n",
    "            'id': self._id_counter,\n",
    "            'sensor_id': sensor_id,\n",
    "            'value': value,\n",
    "            'unit': unit\n",
    "        }\n",
    "        self.readings.append(reading)\n",
    "        return self._id_counter\n",
    "    \n",
    "    def get_readings(self, sensor_id=None, limit=100):\n",
    "        \"\"\"Get readings, optionally filtered by sensor.\"\"\"\n",
    "        if sensor_id:\n",
    "            result = [r for r in self.readings if r['sensor_id'] == sensor_id]\n",
    "        else:\n",
    "            result = self.readings\n",
    "        return result[:limit]\n",
    "    \n",
    "    def get_statistics(self, sensor_id):\n",
    "        \"\"\"Calculate statistics for a sensor.\"\"\"\n",
    "        values = [r['value'] for r in self.readings \n",
    "                  if r['sensor_id'] == sensor_id]\n",
    "        \n",
    "        if not values:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'sensor_id': sensor_id,\n",
    "            'count': len(values),\n",
    "            'min': min(values),\n",
    "            'max': max(values),\n",
    "            'avg': sum(values) / len(values)\n",
    "        }\n",
    "    \n",
    "    def get_all_statistics(self):\n",
    "        \"\"\"Get statistics for all sensors.\"\"\"\n",
    "        sensor_ids = set(r['sensor_id'] for r in self.readings)\n",
    "        return [self.get_statistics(sid) for sid in sensor_ids]\n",
    "\n",
    "# Test the database wrapper\n",
    "db = SensorDatabase()\n",
    "\n",
    "# Add readings\n",
    "for i in range(5):\n",
    "    db.add_reading('TEMP_001', 25.0 + i * 0.2, 'celsius')\n",
    "\n",
    "for i in range(3):\n",
    "    db.add_reading('PRES_001', 1013.0 + i * 0.1, 'hPa')\n",
    "\n",
    "# Get readings for specific sensor\n",
    "print(\"TEMP_001 readings:\")\n",
    "for r in db.get_readings('TEMP_001'):\n",
    "    print(f\"  {r}\")\n",
    "\n",
    "# Get statistics\n",
    "print(\"\\nAll sensor statistics:\")\n",
    "for stats in db.get_all_statistics():\n",
    "    print(f\"  {stats['sensor_id']}: avg={stats['avg']:.2f}, count={stats['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ff013",
   "metadata": {},
   "source": [
    "> üí° **Note:** Always use **parameterized queries** (`?` placeholders) to prevent SQL injection attacks. Never concatenate user input directly into SQL strings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c5db9c",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Data Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525b42b",
   "metadata": {},
   "source": [
    "Always validate external data before processing. Invalid data can cause crashes, security issues, and incorrect results. Good validation catches problems early and provides clear error messages.\n",
    "\n",
    "### Basic Validation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703003fd",
   "metadata": {},
   "source": [
    "**Figure 6.1: Field Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sensor_id(sensor_id):\n",
    "    \"\"\"Validate sensor ID format.\"\"\"\n",
    "    if not isinstance(sensor_id, str):\n",
    "        return False, \"Sensor ID must be a string\"\n",
    "    if len(sensor_id) < 3:\n",
    "        return False, \"Sensor ID too short (min 3 chars)\"\n",
    "    if not sensor_id.replace('_', '').isalnum():\n",
    "        return False, \"Sensor ID contains invalid characters\"\n",
    "    return True, \"Valid\"\n",
    "\n",
    "def validate_temperature(value, min_val=-40, max_val=85):\n",
    "    \"\"\"Validate temperature reading.\"\"\"\n",
    "    if not isinstance(value, (int, float)):\n",
    "        return False, \"Temperature must be a number\"\n",
    "    if value < min_val or value > max_val:\n",
    "        return False, f\"Temperature out of range [{min_val}, {max_val}]\"\n",
    "    return True, \"Valid\"\n",
    "\n",
    "def validate_timestamp(timestamp):\n",
    "    \"\"\"Validate timestamp format.\"\"\"\n",
    "    import re\n",
    "    pattern = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$'\n",
    "    if not isinstance(timestamp, str):\n",
    "        return False, \"Timestamp must be a string\"\n",
    "    if not re.match(pattern, timestamp):\n",
    "        return False, \"Invalid format (expected: YYYY-MM-DD HH:MM:SS)\"\n",
    "    return True, \"Valid\"\n",
    "\n",
    "# Test validation\n",
    "test_cases = [\n",
    "    ('TEMP_001', 25.5, '2025-01-15 10:00:00'),  # Valid\n",
    "    ('T', 25.5, '2025-01-15 10:00:00'),          # Invalid ID\n",
    "    ('TEMP_002', 100, '2025-01-15 10:00:00'),    # Out of range\n",
    "    ('TEMP_003', 25.5, '2025/01/15'),            # Invalid timestamp\n",
    "]\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(\"-\" * 60)\n",
    "for sensor_id, temp, ts in test_cases:\n",
    "    id_ok, id_msg = validate_sensor_id(sensor_id)\n",
    "    temp_ok, temp_msg = validate_temperature(temp)\n",
    "    ts_ok, ts_msg = validate_timestamp(ts)\n",
    "    \n",
    "    status = \"‚úì\" if all([id_ok, temp_ok, ts_ok]) else \"‚úó\"\n",
    "    print(f\"{status} ID={sensor_id}, Temp={temp}, TS={ts}\")\n",
    "    if not id_ok: print(f\"   ‚Üí ID: {id_msg}\")\n",
    "    if not temp_ok: print(f\"   ‚Üí Temp: {temp_msg}\")\n",
    "    if not ts_ok: print(f\"   ‚Üí TS: {ts_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a97123",
   "metadata": {},
   "source": [
    "### Validation with Dataclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bb85b",
   "metadata": {},
   "source": [
    "**Figure 6.2: Structured Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List\n",
    "\n",
    "class ValidationError(Exception):\n",
    "    \"\"\"Raised when data validation fails.\"\"\"\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class SensorReading:\n",
    "    \"\"\"Validated sensor reading.\"\"\"\n",
    "    sensor_id: str\n",
    "    value: float\n",
    "    unit: str\n",
    "    timestamp: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate fields after initialization.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Validate sensor_id\n",
    "        if not self.sensor_id or len(self.sensor_id) < 3:\n",
    "            errors.append(\"sensor_id must be at least 3 characters\")\n",
    "        \n",
    "        # Validate value\n",
    "        if not isinstance(self.value, (int, float)):\n",
    "            errors.append(\"value must be a number\")\n",
    "        elif self.value < -1000 or self.value > 10000:\n",
    "            errors.append(\"value out of reasonable range\")\n",
    "        \n",
    "        # Validate unit\n",
    "        valid_units = ['celsius', 'fahrenheit', 'hPa', 'percent', 'psi']\n",
    "        if self.unit not in valid_units:\n",
    "            errors.append(f\"unit must be one of: {valid_units}\")\n",
    "        \n",
    "        if errors:\n",
    "            raise ValidationError(\"; \".join(errors))\n",
    "\n",
    "# Test validation\n",
    "test_data = [\n",
    "    {'sensor_id': 'TEMP_001', 'value': 25.5, 'unit': 'celsius'},\n",
    "    {'sensor_id': 'T', 'value': 25.5, 'unit': 'celsius'},\n",
    "    {'sensor_id': 'TEMP_002', 'value': 25.5, 'unit': 'kelvin'},\n",
    "]\n",
    "\n",
    "print(\"Creating validated readings:\")\n",
    "for data in test_data:\n",
    "    try:\n",
    "        reading = SensorReading(**data)\n",
    "        print(f\"  ‚úì {reading}\")\n",
    "    except ValidationError as e:\n",
    "        print(f\"  ‚úó {data} -> {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd10b2",
   "metadata": {},
   "source": [
    "### Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223e6d9",
   "metadata": {},
   "source": [
    "**Figure 6.3: Schema-Based Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaValidator:\n",
    "    \"\"\"Validate data against a schema definition.\"\"\"\n",
    "    \n",
    "    def __init__(self, schema):\n",
    "        self.schema = schema\n",
    "    \n",
    "    def validate(self, data):\n",
    "        \"\"\"Validate data against schema.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check required fields\n",
    "        for field, rules in self.schema.items():\n",
    "            if rules.get('required', False) and field not in data:\n",
    "                errors.append(f\"Missing required field: {field}\")\n",
    "                continue\n",
    "            \n",
    "            if field not in data:\n",
    "                continue\n",
    "            \n",
    "            value = data[field]\n",
    "            \n",
    "            # Type check\n",
    "            expected_type = rules.get('type')\n",
    "            if expected_type and not isinstance(value, expected_type):\n",
    "                errors.append(f\"{field}: expected {expected_type.__name__}, got {type(value).__name__}\")\n",
    "            \n",
    "            # Range check\n",
    "            if 'min' in rules and value < rules['min']:\n",
    "                errors.append(f\"{field}: value {value} below minimum {rules['min']}\")\n",
    "            if 'max' in rules and value > rules['max']:\n",
    "                errors.append(f\"{field}: value {value} above maximum {rules['max']}\")\n",
    "            \n",
    "            # Allowed values\n",
    "            if 'allowed' in rules and value not in rules['allowed']:\n",
    "                errors.append(f\"{field}: '{value}' not in allowed values {rules['allowed']}\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "# Define schema\n",
    "sensor_schema = {\n",
    "    'sensor_id': {'required': True, 'type': str},\n",
    "    'value': {'required': True, 'type': (int, float), 'min': -100, 'max': 200},\n",
    "    'unit': {'required': True, 'type': str, 'allowed': ['celsius', 'hPa', 'percent']},\n",
    "}\n",
    "\n",
    "validator = SchemaValidator(sensor_schema)\n",
    "\n",
    "# Test data\n",
    "test_records = [\n",
    "    {'sensor_id': 'TEMP_001', 'value': 25.5, 'unit': 'celsius'},\n",
    "    {'sensor_id': 'TEMP_002', 'value': 250, 'unit': 'celsius'},\n",
    "    {'value': 25.5, 'unit': 'kelvin'},\n",
    "]\n",
    "\n",
    "print(\"Schema Validation Results:\")\n",
    "for record in test_records:\n",
    "    valid, errors = validator.validate(record)\n",
    "    status = \"‚úì\" if valid else \"‚úó\"\n",
    "    print(f\"\\n{status} {record}\")\n",
    "    for error in errors:\n",
    "        print(f\"   ‚Üí {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995670e4",
   "metadata": {},
   "source": [
    "> üí° **Note:** Validate data as early as possible - at the point of entry into your system. This prevents invalid data from propagating through your application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e2543",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Building Data Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb005137",
   "metadata": {},
   "source": [
    "A data pipeline processes data through a series of stages: extraction, transformation, validation, and loading (ETL). Well-designed pipelines are modular, testable, and handle errors gracefully.\n",
    "\n",
    "### Simple Pipeline Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe345c",
   "metadata": {},
   "source": [
    "**Figure 7.1: Basic Data Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Simple data processing pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stages = []\n",
    "        self.stats = {'input': 0, 'output': 0, 'errors': 0}\n",
    "    \n",
    "    def add_stage(self, stage_func, name=None):\n",
    "        \"\"\"Add a processing stage.\"\"\"\n",
    "        self.stages.append({\n",
    "            'func': stage_func,\n",
    "            'name': name or stage_func.__name__\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def process(self, data):\n",
    "        \"\"\"Process data through all stages.\"\"\"\n",
    "        self.stats['input'] = len(data)\n",
    "        results = []\n",
    "        \n",
    "        for item in data:\n",
    "            try:\n",
    "                processed = item\n",
    "                for stage in self.stages:\n",
    "                    processed = stage['func'](processed)\n",
    "                    if processed is None:\n",
    "                        break\n",
    "                if processed is not None:\n",
    "                    results.append(processed)\n",
    "            except Exception as e:\n",
    "                self.stats['errors'] += 1\n",
    "                print(f\"Error processing {item}: {e}\")\n",
    "        \n",
    "        self.stats['output'] = len(results)\n",
    "        return results\n",
    "\n",
    "# Define pipeline stages\n",
    "def clean_data(record):\n",
    "    \"\"\"Clean and normalize data.\"\"\"\n",
    "    record['sensor_id'] = record['sensor_id'].upper().strip()\n",
    "    return record\n",
    "\n",
    "def validate_data(record):\n",
    "    \"\"\"Validate record, return None if invalid.\"\"\"\n",
    "    if 'sensor_id' not in record or 'value' not in record:\n",
    "        return None\n",
    "    if not isinstance(record['value'], (int, float)):\n",
    "        return None\n",
    "    return record\n",
    "\n",
    "def enrich_data(record):\n",
    "    \"\"\"Add computed fields.\"\"\"\n",
    "    record['processed'] = True\n",
    "    return record\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = DataPipeline()\n",
    "pipeline.add_stage(clean_data, 'Clean')\n",
    "pipeline.add_stage(validate_data, 'Validate')\n",
    "pipeline.add_stage(enrich_data, 'Enrich')\n",
    "\n",
    "# Test data\n",
    "raw_data = [\n",
    "    {'sensor_id': 'temp_001', 'value': 25.5},\n",
    "    {'sensor_id': 'TEMP_002', 'value': 26.0},\n",
    "    {'sensor_id': 'TEMP_003'},  # Missing value\n",
    "    {'value': 25.5},  # Missing sensor_id\n",
    "]\n",
    "\n",
    "results = pipeline.process(raw_data)\n",
    "\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"  Input: {pipeline.stats['input']}\")\n",
    "print(f\"  Output: {pipeline.stats['output']}\")\n",
    "print(f\"  Errors: {pipeline.stats['errors']}\")\n",
    "print(\"\\nProcessed records:\")\n",
    "for r in results:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfc3d4",
   "metadata": {},
   "source": [
    "### Pipeline with Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686cc18",
   "metadata": {},
   "source": [
    "**Figure 7.2: Pipeline with Stage Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1449e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingPipeline:\n",
    "    \"\"\"Pipeline with detailed stage logging.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.stages = []\n",
    "        self.stage_stats = []\n",
    "    \n",
    "    def add_stage(self, func, name):\n",
    "        \"\"\"Add a processing stage.\"\"\"\n",
    "        self.stages.append({'func': func, 'name': name})\n",
    "        return self\n",
    "    \n",
    "    def process(self, data):\n",
    "        \"\"\"Process data with logging.\"\"\"\n",
    "        print(f\"Pipeline '{self.name}' starting with {len(data)} records\")\n",
    "        \n",
    "        current_data = data\n",
    "        \n",
    "        for i, stage in enumerate(self.stages):\n",
    "            input_count = len(current_data)\n",
    "            \n",
    "            # Process through stage\n",
    "            results = []\n",
    "            errors = 0\n",
    "            \n",
    "            for item in current_data:\n",
    "                try:\n",
    "                    result = stage['func'](item)\n",
    "                    if result is not None:\n",
    "                        results.append(result)\n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "            \n",
    "            output_count = len(results)\n",
    "            dropped = input_count - output_count - errors\n",
    "            \n",
    "            print(f\"  Stage {i+1} [{stage['name']}]: {input_count} ‚Üí {output_count} ({dropped} dropped, {errors} errors)\")\n",
    "            \n",
    "            current_data = results\n",
    "        \n",
    "        print(f\"Pipeline complete: {len(current_data)} final records\")\n",
    "        return current_data\n",
    "\n",
    "# Pipeline stages\n",
    "def parse_value(record):\n",
    "    record['value'] = float(record['value'])\n",
    "    return record\n",
    "\n",
    "def filter_range(record):\n",
    "    if -50 <= record['value'] <= 100:\n",
    "        return record\n",
    "    return None\n",
    "\n",
    "def add_quality(record):\n",
    "    record['quality'] = 'good' if record['value'] > 0 else 'suspect'\n",
    "    return record\n",
    "\n",
    "# Build and run pipeline\n",
    "pipeline = LoggingPipeline('SensorETL')\n",
    "pipeline.add_stage(parse_value, 'Parse')\n",
    "pipeline.add_stage(filter_range, 'Filter')\n",
    "pipeline.add_stage(add_quality, 'Quality')\n",
    "\n",
    "data = [\n",
    "    {'sensor_id': 'T1', 'value': '25.5'},\n",
    "    {'sensor_id': 'T2', 'value': '200'},  # Out of range\n",
    "    {'sensor_id': 'T3', 'value': '-10'},\n",
    "    {'sensor_id': 'T4', 'value': 'invalid'},  # Will error\n",
    "]\n",
    "\n",
    "results = pipeline.process(data)\n",
    "print(\"\\nFinal results:\")\n",
    "for r in results:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4245b40",
   "metadata": {},
   "source": [
    "> üí° **Note:** Good pipelines log statistics at each stage, making it easy to identify where data quality issues occur and monitor pipeline health!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c860dd4",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Binary Data and Serialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f1eb3",
   "metadata": {},
   "source": [
    "Binary formats are more compact and faster than text formats. The `struct` module handles raw binary data, while `pickle` serializes Python objects. Binary formats are common in embedded systems and high-performance applications.\n",
    "\n",
    "### Working with struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f5890",
   "metadata": {},
   "source": [
    "**Figure 8.1: Binary Data with struct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36122b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "# Pack data into binary format\n",
    "# Format: sensor_id (int), temperature (float), pressure (float)\n",
    "sensor_id = 1\n",
    "temperature = 25.5\n",
    "pressure = 1013.25\n",
    "\n",
    "# Format string: i = int (4 bytes), f = float (4 bytes)\n",
    "format_str = 'iff'\n",
    "binary_data = struct.pack(format_str, sensor_id, temperature, pressure)\n",
    "\n",
    "print(f\"Format: {format_str}\")\n",
    "print(f\"Record size: {struct.calcsize(format_str)} bytes\")\n",
    "print(f\"Packed data (hex): {binary_data.hex()}\")\n",
    "print(f\"Packed data (bytes): {len(binary_data)} bytes\")\n",
    "\n",
    "# Unpack the binary data\n",
    "unpacked = struct.unpack(format_str, binary_data)\n",
    "print(f\"\\nUnpacked values:\")\n",
    "print(f\"  sensor_id: {unpacked[0]}\")\n",
    "print(f\"  temperature: {unpacked[1]:.1f}\")\n",
    "print(f\"  pressure: {unpacked[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1065d76",
   "metadata": {},
   "source": [
    "### Format Characters Reference\n",
    "\n",
    "| Format | C Type | Python Type | Size (bytes) |\n",
    "| --- | --- | --- | --- |\n",
    "| `b` | signed char | int | 1 |\n",
    "| `B` | unsigned char | int | 1 |\n",
    "| `h` | short | int | 2 |\n",
    "| `H` | unsigned short | int | 2 |\n",
    "| `i` | int | int | 4 |\n",
    "| `I` | unsigned int | int | 4 |\n",
    "| `f` | float | float | 4 |\n",
    "| `d` | double | float | 8 |\n",
    "\n",
    "### Binary Sensor Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29092b3e",
   "metadata": {},
   "source": [
    "**Figure 8.2: Binary Data Logger Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import time\n",
    "\n",
    "class BinarySensorLogger:\n",
    "    \"\"\"Log sensor data in compact binary format.\"\"\"\n",
    "    \n",
    "    # Record format: timestamp (double), sensor_id (int), value (float)\n",
    "    RECORD_FORMAT = 'dif'\n",
    "    RECORD_SIZE = struct.calcsize(RECORD_FORMAT)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buffer = bytearray()\n",
    "        self.record_count = 0\n",
    "    \n",
    "    def write(self, timestamp, sensor_id, value):\n",
    "        \"\"\"Write a reading to the log.\"\"\"\n",
    "        record = struct.pack(self.RECORD_FORMAT, timestamp, sensor_id, value)\n",
    "        self.buffer.extend(record)\n",
    "        self.record_count += 1\n",
    "    \n",
    "    def read_all(self):\n",
    "        \"\"\"Read all records from the log.\"\"\"\n",
    "        records = []\n",
    "        offset = 0\n",
    "        \n",
    "        while offset + self.RECORD_SIZE <= len(self.buffer):\n",
    "            data = self.buffer[offset:offset + self.RECORD_SIZE]\n",
    "            timestamp, sensor_id, value = struct.unpack(self.RECORD_FORMAT, data)\n",
    "            records.append({\n",
    "                'timestamp': timestamp,\n",
    "                'sensor_id': sensor_id,\n",
    "                'value': value\n",
    "            })\n",
    "            offset += self.RECORD_SIZE\n",
    "        \n",
    "        return records\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Return buffer statistics.\"\"\"\n",
    "        return {\n",
    "            'records': self.record_count,\n",
    "            'buffer_size': len(self.buffer),\n",
    "            'bytes_per_record': self.RECORD_SIZE\n",
    "        }\n",
    "\n",
    "# Test the binary logger\n",
    "logger = BinarySensorLogger()\n",
    "\n",
    "# Write some readings\n",
    "base_time = time.time()\n",
    "for i in range(10):\n",
    "    logger.write(base_time + i, 1, 25.0 + i * 0.5)\n",
    "\n",
    "# Get statistics\n",
    "stats = logger.get_stats()\n",
    "print(\"Binary Logger Statistics:\")\n",
    "print(f\"  Records: {stats['records']}\")\n",
    "print(f\"  Buffer size: {stats['buffer_size']} bytes\")\n",
    "print(f\"  Bytes per record: {stats['bytes_per_record']}\")\n",
    "\n",
    "# Compare to JSON\n",
    "import json\n",
    "json_size = len(json.dumps(logger.read_all()))\n",
    "print(f\"\\nSize comparison:\")\n",
    "print(f\"  Binary: {stats['buffer_size']} bytes\")\n",
    "print(f\"  JSON: {json_size} bytes\")\n",
    "print(f\"  Savings: {(1 - stats['buffer_size']/json_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a632d",
   "metadata": {},
   "source": [
    "### Data Format Summary\n",
    "\n",
    "| Format | Module | Read | Write | Best For |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| CSV | `csv` | `csv.reader()` | `csv.writer()` | Tabular data, spreadsheets |\n",
    "| JSON | `json` | `json.load()` | `json.dump()` | APIs, configs, nested data |\n",
    "| XML | `xml.etree` | `ET.parse()` | `tree.write()` | Industry standards, complex structure |\n",
    "| SQLite | `sqlite3` | `cursor.fetchall()` | `cursor.execute()` | Relational data, queries |\n",
    "| Binary | `struct` | `struct.unpack()` | `struct.pack()` | Embedded systems, performance |\n",
    "\n",
    "> üí° **Note:** Never use `pickle` to load data from untrusted sources - it can execute arbitrary code! Use JSON or other safe formats for external data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b46522",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ùå Common Mistakes to Avoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f546feb",
   "metadata": {},
   "source": [
    "These are the most frequent errors students make with external data formats. Study them before the exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726580a",
   "metadata": {},
   "source": [
    "**Using `json.load()` vs `json.loads()`**\n",
    "\n",
    "`json.load(file_object)` reads from a file. `json.loads(string)` parses a string. Mixing them up gives `TypeError` or `AttributeError`. Same for `json.dump()` vs `json.dumps()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9887e6",
   "metadata": {},
   "source": [
    "**Not handling encoding when reading CSV/JSON**\n",
    "\n",
    "`open(\"data.csv\")` uses system default encoding. Files from other systems may use UTF-8 with BOM, Latin-1, etc. Always specify: `open(\"data.csv\", encoding=\"utf-8-sig\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf1b12",
   "metadata": {},
   "source": [
    "**Assuming JSON preserves Python types**\n",
    "\n",
    "                    JSON has no `tuple`, `set`, or `datetime`. Tuples become lists, sets can't be serialized at all, and dates need manual conversion. Always validate after loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50f1f0",
   "metadata": {},
   "source": [
    "**Not validating external data**\n",
    "\n",
    "                    Data from files or APIs can be missing fields, have wrong types, or contain unexpected values. Never trust external data ‚Äî always validate before processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fa63c",
   "metadata": {},
   "source": [
    "**Hardcoding file paths**\n",
    "\n",
    "`open(\"C:\\\\Users\\\\student\\\\data.csv\")` breaks on any other machine. Use `os.path.join()` or `pathlib.Path()` for portable paths, and relative paths where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cacd22d",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d61eb",
   "metadata": {},
   "source": [
    "### Exercise 1: CSV Reader  (Easy)\n",
    "\n",
    "Create a function that reads CSV data and returns a list of dictionaries.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Record 1: {'name': 'Alice', 'score': '95'}\n",
    "Record 2: {'name': 'Bob', 'score': '87'}\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Use csv.DictReader\n",
    "- Use StringIO to create a file-like object from string\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bece2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX1]\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "csv_data = \"\"\"name,score\n",
    "Alice,95\n",
    "Bob,87\"\"\"\n",
    "\n",
    "def read_csv_to_dicts(csv_string):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "records = read_csv_to_dicts(csv_data)\n",
    "for i, record in enumerate(records, 1):\n",
    "    print(f\"Record {i}: {record}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71295b2b",
   "metadata": {},
   "source": [
    "### Exercise 2: JSON Parser  (Easy)\n",
    "\n",
    "Parse JSON data and extract sensor values.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "TEMP_001: 25.5 celsius\n",
    "PRES_001: 1013.25 hPa\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Use `json.loads()` to parse JSON string\n",
    "- Access nested data: `data['sensors']`\n",
    "- Loop through list and access dict keys\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24597531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX2]\n",
    "import json\n",
    "\n",
    "json_data = '''\n",
    "{\n",
    "    \"sensors\": [\n",
    "        {\"id\": \"TEMP_001\", \"value\": 25.5, \"unit\": \"celsius\"},\n",
    "        {\"id\": \"PRES_001\", \"value\": 1013.25, \"unit\": \"hPa\"}\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Parse and print sensor values\n",
    "# Your code here\n",
    "data = json.loads(json_data)\n",
    "for sensor in data['sensors']:\n",
    "    print(f\"{sensor['id']}: {sensor['value']} {sensor['unit']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc519c0a",
   "metadata": {},
   "source": [
    "### Exercise 3: Data Validator  (Medium)\n",
    "\n",
    "Create a validate_reading function that checks sensor_id (non-empty string) and value (number in range -50 to 150).\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "TEMP_001, 25.5: Valid\n",
    "TEMP_002, 200: Invalid - value out of range\n",
    ", 25.5: Invalid - missing sensor_id\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Check if sensor_id exists and is non-empty\n",
    "- Use isinstance() to check if value is a number\n",
    "- Return tuple of (is_valid, message)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c993f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX3]\n",
    "def validate_reading(sensor_id, value):\n",
    "    \"\"\"Validate sensor reading. Returns (is_valid, message).\"\"\"\n",
    "    # Your code here\n",
    "    if not sensor_id:\n",
    "        return False, \"missing sensor_id\"\n",
    "    if not isinstance(value, (int, float)):\n",
    "        return False, \"value must be a number\"\n",
    "    if value < -50 or value > 150:\n",
    "        return False, \"value out of range\"\n",
    "    return True, \"Valid\"\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    ('TEMP_001', 25.5),\n",
    "    ('TEMP_002', 200),\n",
    "    ('', 25.5),\n",
    "]\n",
    "\n",
    "for sensor_id, value in test_cases:\n",
    "    valid, msg = validate_reading(sensor_id, value)\n",
    "    print(f\"{sensor_id}, {value}: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f69aa5",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple Database  (Medium)\n",
    "\n",
    "Create a SimpleDB class with insert, select_all, and select_by_sensor methods.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Inserted 3 records\n",
    "All: 3 records\n",
    "TEMP_001: 2 records\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Store records in `self.records = []`\n",
    "- insert: append dict with auto-increment id\n",
    "- select_by_sensor: filter with list comprehension\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38488ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX4]\n",
    "class SimpleDB:\n",
    "    def __init__(self):\n",
    "        self.records = []\n",
    "        self._id = 0\n",
    "    \n",
    "    def insert(self, sensor_id, value, unit):\n",
    "        self._id += 1\n",
    "        self.records.append({\n",
    "            'id': self._id,\n",
    "            'sensor_id': sensor_id,\n",
    "            'value': value,\n",
    "            'unit': unit\n",
    "        })\n",
    "        return self._id\n",
    "    \n",
    "    def select_all(self):\n",
    "        return self.records\n",
    "    \n",
    "    def select_by_sensor(self, sensor_id):\n",
    "        return [r for r in self.records if r['sensor_id'] == sensor_id]\n",
    "\n",
    "# Test\n",
    "db = SimpleDB()\n",
    "db.insert('TEMP_001', 25.5, 'celsius')\n",
    "db.insert('TEMP_001', 25.7, 'celsius')\n",
    "db.insert('PRES_001', 1013.25, 'hPa')\n",
    "\n",
    "print(f\"Inserted 3 records\")\n",
    "print(f\"All: {len(db.select_all())} records\")\n",
    "print(f\"TEMP_001: {len(db.select_by_sensor('TEMP_001'))} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4129f",
   "metadata": {},
   "source": [
    "### Exercise 5: CSV Statistics  (Medium)\n",
    "\n",
    "Calculate min, max, and average from CSV temperature data.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Statistics: min=22.1, max=26.0, avg=24.50\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Extract values: `[float(row['temp']) for row in reader]`\n",
    "- Use `min()`, `max()`, `sum()/len()`\n",
    "- Return dict with min, max, avg keys\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX5]\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "csv_data = \"\"\"sensor,temperature\n",
    "T1,25.5\n",
    "T1,26.0\n",
    "T2,22.1\n",
    "T2,24.5\"\"\"\n",
    "\n",
    "def calculate_stats(csv_string):\n",
    "    reader = csv.DictReader(StringIO(csv_string))\n",
    "    temps = [float(row['temperature']) for row in reader]\n",
    "    return {\n",
    "        'min': min(temps),\n",
    "        'max': max(temps),\n",
    "        'avg': sum(temps) / len(temps)\n",
    "    }\n",
    "\n",
    "stats = calculate_stats(csv_data)\n",
    "print(f\"Statistics: min={stats['min']}, max={stats['max']}, avg={stats['avg']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418d486",
   "metadata": {},
   "source": [
    "### Exercise 6: JSON Configuration  (Medium)\n",
    "\n",
    "Create a Config class that loads settings from JSON and provides get/set methods.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- __init__: `self.data = json.loads(json_str)`\n",
    "- get: `return self.data.get(key, default)`\n",
    "- to_json: `json.dumps(self.data, indent=2)`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX6]\n",
    "import json\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, json_str=None):\n",
    "        self.data = json.loads(json_str) if json_str else {}\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        return self.data.get(key, default)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        self.data[key] = value\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.data, indent=2)\n",
    "\n",
    "# Test\n",
    "config = Config('{\"debug\": true, \"timeout\": 30}')\n",
    "print(f\"Debug: {config.get('debug')}\")\n",
    "print(f\"Timeout: {config.get('timeout')}\")\n",
    "config.set('version', '1.0')\n",
    "print(f\"Config:\\n{config.to_json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafa305",
   "metadata": {},
   "source": [
    "### Exercise 7: XML Parser  (Medium)\n",
    "\n",
    "Parse XML and extract sensor readings into a list of dictionaries.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Parse: `ET.fromstring(xml_string)`\n",
    "- Find elements: `root.findall('sensor')`\n",
    "- Get attribute: `sensor.get('id')`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX7]\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "xml_data = '''\n",
    "<sensors>\n",
    "    <sensor id=\"T1\"><value>25.5</value></sensor>\n",
    "    <sensor id=\"T2\"><value>26.0</value></sensor>\n",
    "</sensors>\n",
    "'''\n",
    "\n",
    "def parse_sensors(xml_string):\n",
    "    root = ET.fromstring(xml_string)\n",
    "    readings = []\n",
    "    for sensor in root.findall('sensor'):\n",
    "        readings.append({\n",
    "            'id': sensor.get('id'),\n",
    "            'value': float(sensor.find('value').text)\n",
    "        })\n",
    "    return readings\n",
    "\n",
    "sensors = parse_sensors(xml_data)\n",
    "for s in sensors:\n",
    "    print(f\"{s['id']}: {s['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ce1b9",
   "metadata": {},
   "source": [
    "### Exercise 8: Binary Packer  (Medium)\n",
    "\n",
    "Use struct to pack and unpack sensor data (id: int, value: float, status: byte).\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Format string: 'i'=int, 'f'=float, 'B'=byte\n",
    "- Pack: `struct.pack(FORMAT, id, val, stat)`\n",
    "- Unpack: `struct.unpack(FORMAT, data)`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854331a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX8]\n",
    "import struct\n",
    "\n",
    "# Format: int (4 bytes), float (4 bytes), byte (1 byte)\n",
    "FORMAT = 'ifB'\n",
    "\n",
    "def pack_reading(sensor_id, value, status):\n",
    "    return struct.pack(FORMAT, sensor_id, value, status)\n",
    "\n",
    "def unpack_reading(data):\n",
    "    return struct.unpack(FORMAT, data)\n",
    "\n",
    "# Pack data\n",
    "packed = pack_reading(1, 25.5, 1)\n",
    "print(f\"Packed size: {len(packed)} bytes\")\n",
    "print(f\"Packed (hex): {packed.hex()}\")\n",
    "\n",
    "# Unpack data\n",
    "sid, val, stat = unpack_reading(packed)\n",
    "print(f\"Unpacked: id={sid}, value={val}, status={stat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39769a6",
   "metadata": {},
   "source": [
    "### Exercise 9: Data Pipeline  (Challenge)\n",
    "\n",
    "Create a data pipeline that validates, cleans, and transforms sensor readings.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Clean: uppercase sensor_id, convert value to float\n",
    "- Validate: check sensor_id exists, value in range\n",
    "- Transform: add processed flag and timestamp\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ab0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX9]\n",
    "class DataPipeline:\n",
    "    def __init__(self):\n",
    "        self.stats = {'input': 0, 'output': 0, 'dropped': 0}\n",
    "    \n",
    "    def clean(self, record):\n",
    "        record['sensor_id'] = record.get('sensor_id', '').upper()\n",
    "        try:\n",
    "            record['value'] = float(record.get('value', 0))\n",
    "        except:\n",
    "            record['value'] = None\n",
    "        return record\n",
    "    \n",
    "    def validate(self, record):\n",
    "        if not record['sensor_id']:\n",
    "            return None\n",
    "        if record['value'] is None or record['value'] < -50 or record['value'] > 150:\n",
    "            return None\n",
    "        return record\n",
    "    \n",
    "    def transform(self, record):\n",
    "        record['processed'] = True\n",
    "        return record\n",
    "    \n",
    "    def process(self, data):\n",
    "        self.stats['input'] = len(data)\n",
    "        results = []\n",
    "        for item in data:\n",
    "            item = self.clean(item)\n",
    "            item = self.validate(item)\n",
    "            if item:\n",
    "                item = self.transform(item)\n",
    "                results.append(item)\n",
    "            else:\n",
    "                self.stats['dropped'] += 1\n",
    "        self.stats['output'] = len(results)\n",
    "        return results\n",
    "\n",
    "# Test\n",
    "pipeline = DataPipeline()\n",
    "data = [\n",
    "    {'sensor_id': 'temp_001', 'value': '25.5'},\n",
    "    {'sensor_id': 'temp_002', 'value': 200},\n",
    "    {'sensor_id': '', 'value': 25},\n",
    "]\n",
    "\n",
    "results = pipeline.process(data)\n",
    "print(f\"Input: {pipeline.stats['input']}, Output: {pipeline.stats['output']}, Dropped: {pipeline.stats['dropped']}\")\n",
    "for r in results:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3b02a",
   "metadata": {},
   "source": [
    "### Exercise 10: API Client  (Challenge)\n",
    "\n",
    "Create a mock API client with GET and POST methods and error handling.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Store data in `self.data = []`\n",
    "- GET: check endpoint and return matching data\n",
    "- Return dict with 'status' and 'data' or 'error'\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX10]\n",
    "class MockAPI:\n",
    "    def __init__(self):\n",
    "        self.data = [\n",
    "            {'id': 'T1', 'value': 25.5},\n",
    "            {'id': 'T2', 'value': 26.0}\n",
    "        ]\n",
    "    \n",
    "    def get(self, endpoint):\n",
    "        if endpoint == '/sensors':\n",
    "            return {'status': 200, 'data': self.data}\n",
    "        elif endpoint.startswith('/sensors/'):\n",
    "            sid = endpoint.split('/')[-1]\n",
    "            for s in self.data:\n",
    "                if s['id'] == sid:\n",
    "                    return {'status': 200, 'data': s}\n",
    "            return {'status': 404, 'error': 'Not found'}\n",
    "        return {'status': 400, 'error': 'Bad request'}\n",
    "    \n",
    "    def post(self, endpoint, data):\n",
    "        if endpoint == '/sensors':\n",
    "            self.data.append(data)\n",
    "            return {'status': 201, 'data': data}\n",
    "        return {'status': 400, 'error': 'Bad request'}\n",
    "\n",
    "# Test\n",
    "api = MockAPI()\n",
    "\n",
    "# GET all\n",
    "resp = api.get('/sensors')\n",
    "print(f\"GET /sensors: {resp['status']}, {len(resp['data'])} sensors\")\n",
    "\n",
    "# GET one\n",
    "resp = api.get('/sensors/T1')\n",
    "print(f\"GET /sensors/T1: {resp}\")\n",
    "\n",
    "# POST new\n",
    "resp = api.post('/sensors', {'id': 'T3', 'value': 27.0})\n",
    "print(f\"POST: {resp['status']}, added {resp['data']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f7bfe",
   "metadata": {},
   "source": [
    "### Exercise 11: Schema Validator  (Challenge)\n",
    "\n",
    "Implement a schema validator that checks required fields, types, and value ranges.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Schema: dict of field names to rules\n",
    "- Check required, type with `isinstance()`\n",
    "- Return tuple: `(is_valid, error_list)`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX11]\n",
    "class SchemaValidator:\n",
    "    def __init__(self, schema):\n",
    "        self.schema = schema\n",
    "    \n",
    "    def validate(self, data):\n",
    "        errors = []\n",
    "        for field, rules in self.schema.items():\n",
    "            # Check required\n",
    "            if rules.get('required') and field not in data:\n",
    "                errors.append(f\"Missing: {field}\")\n",
    "                continue\n",
    "            if field not in data:\n",
    "                continue\n",
    "            \n",
    "            value = data[field]\n",
    "            \n",
    "            # Check type\n",
    "            if 'type' in rules and not isinstance(value, rules['type']):\n",
    "                errors.append(f\"{field}: wrong type\")\n",
    "            \n",
    "            # Check range\n",
    "            if 'min' in rules and value < rules['min']:\n",
    "                errors.append(f\"{field}: below min\")\n",
    "            if 'max' in rules and value > rules['max']:\n",
    "                errors.append(f\"{field}: above max\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "# Test\n",
    "schema = {\n",
    "    'sensor_id': {'required': True, 'type': str},\n",
    "    'value': {'required': True, 'type': (int, float), 'min': -50, 'max': 150}\n",
    "}\n",
    "\n",
    "validator = SchemaValidator(schema)\n",
    "\n",
    "test_data = [\n",
    "    {'sensor_id': 'T1', 'value': 25.5},\n",
    "    {'sensor_id': 'T2', 'value': 200},\n",
    "    {'value': 25.5},\n",
    "]\n",
    "\n",
    "for data in test_data:\n",
    "    valid, errors = validator.validate(data)\n",
    "    print(f\"{data}: {'‚úì' if valid else '‚úó ' + str(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ad6ae",
   "metadata": {},
   "source": [
    "### Exercise 12: Data Converter  (Challenge)\n",
    "\n",
    "Create functions to convert between CSV, JSON, and XML formats.\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- CSV to JSON: DictReader ‚Üí list ‚Üí json.dumps\n",
    "- JSON to CSV: json.loads ‚Üí DictWriter\n",
    "- Use StringIO for string ‚Üî file conversion\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EX12]\n",
    "import csv\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import StringIO\n",
    "\n",
    "def csv_to_json(csv_str):\n",
    "    reader = csv.DictReader(StringIO(csv_str))\n",
    "    return json.dumps(list(reader), indent=2)\n",
    "\n",
    "def json_to_csv(json_str, fieldnames):\n",
    "    data = json.loads(json_str)\n",
    "    output = StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "    return output.getvalue()\n",
    "\n",
    "# Test CSV to JSON\n",
    "csv_data = \"\"\"id,value\n",
    "T1,25.5\n",
    "T2,26.0\"\"\"\n",
    "\n",
    "json_output = csv_to_json(csv_data)\n",
    "print(\"CSV to JSON:\")\n",
    "print(json_output)\n",
    "\n",
    "# Test JSON to CSV\n",
    "json_data = '[{\"id\": \"T1\", \"value\": 25.5}, {\"id\": \"T2\", \"value\": 26.0}]'\n",
    "csv_output = json_to_csv(json_data, ['id', 'value'])\n",
    "print(\"\\nJSON to CSV:\")\n",
    "print(csv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93d311",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Advanced Data Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf98ba",
   "metadata": {},
   "source": [
    "Advanced patterns for building robust, scalable data processing systems in engineering applications.\n",
    "\n",
    "### Repository Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ecaf7",
   "metadata": {},
   "source": [
    "**Figure 9.1: Repository Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3db2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class SensorRepository(ABC):\n",
    "    \"\"\"Abstract repository for sensor data.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def add(self, reading):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_by_id(self, sensor_id):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_all(self):\n",
    "        pass\n",
    "\n",
    "class MemoryRepository(SensorRepository):\n",
    "    \"\"\"In-memory implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "    \n",
    "    def add(self, reading):\n",
    "        self.data.append(reading)\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_by_id(self, sensor_id):\n",
    "        return [r for r in self.data if r['sensor_id'] == sensor_id]\n",
    "    \n",
    "    def get_all(self):\n",
    "        return self.data.copy()\n",
    "\n",
    "# Test repository\n",
    "repo = MemoryRepository()\n",
    "repo.add({'sensor_id': 'T1', 'value': 25.5})\n",
    "repo.add({'sensor_id': 'T1', 'value': 26.0})\n",
    "repo.add({'sensor_id': 'T2', 'value': 22.1})\n",
    "\n",
    "print(f\"All readings: {len(repo.get_all())}\")\n",
    "print(f\"T1 readings: {repo.get_by_id('T1')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709a98d",
   "metadata": {},
   "source": [
    "### Observer Pattern for Data Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271aedc",
   "metadata": {},
   "source": [
    "**Figure 9.2: Observer Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29590919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSubject:\n",
    "    \"\"\"Observable data source.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._observers = []\n",
    "    \n",
    "    def subscribe(self, observer):\n",
    "        self._observers.append(observer)\n",
    "    \n",
    "    def unsubscribe(self, observer):\n",
    "        self._observers.remove(observer)\n",
    "    \n",
    "    def notify(self, data):\n",
    "        for observer in self._observers:\n",
    "            observer(data)\n",
    "\n",
    "class SensorMonitor:\n",
    "    \"\"\"Sensor data monitor with observers.\"\"\"\n",
    "    \n",
    "    def __init__(self, sensor_id):\n",
    "        self.sensor_id = sensor_id\n",
    "        self.subject = DataSubject()\n",
    "        self.latest_value = None\n",
    "    \n",
    "    def on_reading(self, callback):\n",
    "        self.subject.subscribe(callback)\n",
    "    \n",
    "    def record(self, value):\n",
    "        self.latest_value = value\n",
    "        self.subject.notify({\n",
    "            'sensor_id': self.sensor_id,\n",
    "            'value': value\n",
    "        })\n",
    "\n",
    "# Set up monitoring\n",
    "monitor = SensorMonitor('TEMP_001')\n",
    "\n",
    "# Add observers\n",
    "def log_reading(data):\n",
    "    print(f\"LOG: {data['sensor_id']} = {data['value']}\")\n",
    "\n",
    "def check_alert(data):\n",
    "    if data['value'] > 30:\n",
    "        print(f\"‚ö†Ô∏è ALERT: High temperature!\")\n",
    "\n",
    "monitor.on_reading(log_reading)\n",
    "monitor.on_reading(check_alert)\n",
    "\n",
    "# Record some values\n",
    "for val in [25.5, 28.0, 32.0, 24.0]:\n",
    "    print(f\"Recording: {val}\")\n",
    "    monitor.record(val)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3671bb8",
   "metadata": {},
   "source": [
    "### Data Transform Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db790a4",
   "metadata": {},
   "source": [
    "**Figure 9.3: Transform Chain Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform:\n",
    "    \"\"\"Base transform class.\"\"\"\n",
    "    \n",
    "    def __init__(self, next_transform=None):\n",
    "        self.next = next_transform\n",
    "    \n",
    "    def process(self, data):\n",
    "        result = self.transform(data)\n",
    "        if self.next and result is not None:\n",
    "            return self.next.process(result)\n",
    "        return result\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return data\n",
    "\n",
    "class NormalizeTransform(Transform):\n",
    "    \"\"\"Normalize sensor ID to uppercase.\"\"\"\n",
    "    def transform(self, data):\n",
    "        data['sensor_id'] = data.get('sensor_id', '').upper()\n",
    "        return data\n",
    "\n",
    "class ValidateTransform(Transform):\n",
    "    \"\"\"Validate data, return None if invalid.\"\"\"\n",
    "    def transform(self, data):\n",
    "        if not data.get('sensor_id'):\n",
    "            return None\n",
    "        if not isinstance(data.get('value'), (int, float)):\n",
    "            return None\n",
    "        return data\n",
    "\n",
    "class EnrichTransform(Transform):\n",
    "    \"\"\"Add computed fields.\"\"\"\n",
    "    def transform(self, data):\n",
    "        data['processed'] = True\n",
    "        data['status'] = 'valid'\n",
    "        return data\n",
    "\n",
    "# Build transform chain\n",
    "chain = NormalizeTransform(\n",
    "    ValidateTransform(\n",
    "        EnrichTransform()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Process data\n",
    "test_data = [\n",
    "    {'sensor_id': 'temp_001', 'value': 25.5},\n",
    "    {'sensor_id': '', 'value': 25.5},\n",
    "    {'sensor_id': 'temp_002', 'value': 'invalid'},\n",
    "]\n",
    "\n",
    "print(\"Transform Chain Results:\")\n",
    "for data in test_data:\n",
    "    result = chain.process(data.copy())\n",
    "    print(f\"  Input: {data}\")\n",
    "    print(f\"  Output: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585ef51",
   "metadata": {},
   "source": [
    "### Data Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0821c1a",
   "metadata": {},
   "source": [
    "**Figure 9.4: Rolling Aggregator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aadd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class RollingAggregator:\n",
    "    \"\"\"Calculate rolling statistics over a window.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=10):\n",
    "        self.window_size = window_size\n",
    "        self.values = deque(maxlen=window_size)\n",
    "    \n",
    "    def add(self, value):\n",
    "        self.values.append(value)\n",
    "    \n",
    "    @property\n",
    "    def count(self):\n",
    "        return len(self.values)\n",
    "    \n",
    "    @property\n",
    "    def mean(self):\n",
    "        if not self.values:\n",
    "            return 0\n",
    "        return sum(self.values) / len(self.values)\n",
    "    \n",
    "    @property\n",
    "    def min(self):\n",
    "        return min(self.values) if self.values else 0\n",
    "    \n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.values) if self.values else 0\n",
    "    \n",
    "    def stats(self):\n",
    "        return {\n",
    "            'count': self.count,\n",
    "            'mean': round(self.mean, 2),\n",
    "            'min': self.min,\n",
    "            'max': self.max\n",
    "        }\n",
    "\n",
    "# Test rolling aggregator\n",
    "agg = RollingAggregator(window_size=5)\n",
    "\n",
    "readings = [25.0, 25.5, 26.0, 25.8, 26.2, 27.0, 26.5, 26.0]\n",
    "\n",
    "print(\"Rolling Statistics (window=5):\")\n",
    "for i, value in enumerate(readings):\n",
    "    agg.add(value)\n",
    "    print(f\"  After {value}: {agg.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e47420",
   "metadata": {},
   "source": [
    "### Batch Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03fc5f",
   "metadata": {},
   "source": [
    "**Figure 9.5: Batch Processing Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac765164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProcessor:\n",
    "    \"\"\"Process data in batches for efficiency.\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size=5, process_func=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.process_func = process_func or self._default_process\n",
    "        self.buffer = []\n",
    "        self.results = []\n",
    "        self.batches_processed = 0\n",
    "    \n",
    "    def _default_process(self, batch):\n",
    "        return batch\n",
    "    \n",
    "    def add(self, item):\n",
    "        self.buffer.append(item)\n",
    "        if len(self.buffer) >= self.batch_size:\n",
    "            self._process_batch()\n",
    "    \n",
    "    def _process_batch(self):\n",
    "        if self.buffer:\n",
    "            result = self.process_func(self.buffer)\n",
    "            self.results.extend(result if isinstance(result, list) else [result])\n",
    "            self.batches_processed += 1\n",
    "            print(f\"  Processed batch {self.batches_processed}: {len(self.buffer)} items\")\n",
    "            self.buffer = []\n",
    "    \n",
    "    def flush(self):\n",
    "        self._process_batch()\n",
    "    \n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "# Process function that calculates batch statistics\n",
    "def calculate_batch_stats(batch):\n",
    "    values = [item['value'] for item in batch]\n",
    "    return [{\n",
    "        'batch_avg': sum(values) / len(values),\n",
    "        'batch_count': len(values),\n",
    "        'batch_sensors': list(set(item['sensor_id'] for item in batch))\n",
    "    }]\n",
    "\n",
    "# Test batch processor\n",
    "processor = BatchProcessor(batch_size=3, process_func=calculate_batch_stats)\n",
    "\n",
    "print(\"Adding readings to batch processor:\")\n",
    "readings = [\n",
    "    {'sensor_id': 'T1', 'value': 25.0},\n",
    "    {'sensor_id': 'T1', 'value': 25.5},\n",
    "    {'sensor_id': 'T2', 'value': 26.0},\n",
    "    {'sensor_id': 'T1', 'value': 25.2},\n",
    "    {'sensor_id': 'T2', 'value': 26.5},\n",
    "]\n",
    "\n",
    "for r in readings:\n",
    "    processor.add(r)\n",
    "\n",
    "processor.flush()\n",
    "\n",
    "print(f\"\\nResults: {processor.get_results()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f5dc5",
   "metadata": {},
   "source": [
    "### Data Quality Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726685b9",
   "metadata": {},
   "source": [
    "**Figure 9.6: Data Quality Monitoring**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade4e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityMonitor:\n",
    "    \"\"\"Monitor data quality metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'total': 0,\n",
    "            'valid': 0,\n",
    "            'invalid': 0,\n",
    "            'missing_fields': 0,\n",
    "            'out_of_range': 0,\n",
    "            'type_errors': 0\n",
    "        }\n",
    "        self.issues = []\n",
    "    \n",
    "    def check(self, record, schema):\n",
    "        \"\"\"Check record against schema.\"\"\"\n",
    "        self.metrics['total'] += 1\n",
    "        issues = []\n",
    "        \n",
    "        for field, rules in schema.items():\n",
    "            # Check required\n",
    "            if rules.get('required') and field not in record:\n",
    "                issues.append(f\"Missing: {field}\")\n",
    "                self.metrics['missing_fields'] += 1\n",
    "                continue\n",
    "            \n",
    "            if field not in record:\n",
    "                continue\n",
    "            \n",
    "            value = record[field]\n",
    "            \n",
    "            # Check type\n",
    "            if 'type' in rules and not isinstance(value, rules['type']):\n",
    "                issues.append(f\"Type error: {field}\")\n",
    "                self.metrics['type_errors'] += 1\n",
    "            \n",
    "            # Check range\n",
    "            if isinstance(value, (int, float)):\n",
    "                if 'min' in rules and value < rules['min']:\n",
    "                    issues.append(f\"Below min: {field}\")\n",
    "                    self.metrics['out_of_range'] += 1\n",
    "                if 'max' in rules and value > rules['max']:\n",
    "                    issues.append(f\"Above max: {field}\")\n",
    "                    self.metrics['out_of_range'] += 1\n",
    "        \n",
    "        if issues:\n",
    "            self.metrics['invalid'] += 1\n",
    "            self.issues.append({'record': record, 'issues': issues})\n",
    "        else:\n",
    "            self.metrics['valid'] += 1\n",
    "        \n",
    "        return len(issues) == 0\n",
    "    \n",
    "    def report(self):\n",
    "        valid_pct = (self.metrics['valid'] / self.metrics['total'] * 100) if self.metrics['total'] > 0 else 0\n",
    "        return {\n",
    "            **self.metrics,\n",
    "            'valid_pct': round(valid_pct, 1)\n",
    "        }\n",
    "\n",
    "# Test quality monitor\n",
    "schema = {\n",
    "    'sensor_id': {'required': True, 'type': str},\n",
    "    'value': {'required': True, 'type': (int, float), 'min': -50, 'max': 150}\n",
    "}\n",
    "\n",
    "monitor = DataQualityMonitor()\n",
    "\n",
    "test_records = [\n",
    "    {'sensor_id': 'T1', 'value': 25.5},\n",
    "    {'sensor_id': 'T2', 'value': 200},\n",
    "    {'value': 25.5},\n",
    "    {'sensor_id': 'T3', 'value': 'invalid'},\n",
    "    {'sensor_id': 'T4', 'value': 30.0},\n",
    "]\n",
    "\n",
    "print(\"Checking data quality:\")\n",
    "for record in test_records:\n",
    "    valid = monitor.check(record, schema)\n",
    "    status = \"‚úì\" if valid else \"‚úó\"\n",
    "    print(f\"  {status} {record}\")\n",
    "\n",
    "print(f\"\\nQuality Report: {monitor.report()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096d352",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Data Integration Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881ca24",
   "metadata": {},
   "source": [
    "Patterns for integrating data from multiple sources and formats in engineering systems.\n",
    "\n",
    "### Data Adapter Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0127e85b",
   "metadata": {},
   "source": [
    "**Figure 10.1: Data Adapter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DataAdapter(ABC):\n",
    "    \"\"\"Abstract adapter for different data sources.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def read(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def to_standard_format(self, data):\n",
    "        pass\n",
    "\n",
    "class CSVAdapter(DataAdapter):\n",
    "    \"\"\"Adapter for CSV data.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_data):\n",
    "        self.csv_data = csv_data\n",
    "    \n",
    "    def read(self):\n",
    "        import csv\n",
    "        from io import StringIO\n",
    "        reader = csv.DictReader(StringIO(self.csv_data))\n",
    "        return list(reader)\n",
    "    \n",
    "    def to_standard_format(self, data):\n",
    "        return [{\n",
    "            'sensor_id': row.get('id', row.get('sensor_id', '')),\n",
    "            'value': float(row.get('value', row.get('reading', 0))),\n",
    "            'source': 'csv'\n",
    "        } for row in data]\n",
    "\n",
    "class JSONAdapter(DataAdapter):\n",
    "    \"\"\"Adapter for JSON data.\"\"\"\n",
    "    \n",
    "    def __init__(self, json_data):\n",
    "        self.json_data = json_data\n",
    "    \n",
    "    def read(self):\n",
    "        import json\n",
    "        return json.loads(self.json_data)\n",
    "    \n",
    "    def to_standard_format(self, data):\n",
    "        return [{\n",
    "            'sensor_id': item.get('sensor_id', item.get('name', '')),\n",
    "            'value': float(item.get('value', item.get('reading', 0))),\n",
    "            'source': 'json'\n",
    "        } for item in data]\n",
    "\n",
    "# Test adapters\n",
    "csv_data = \"\"\"id,value\n",
    "T1,25.5\n",
    "T2,26.0\"\"\"\n",
    "\n",
    "json_data = '[{\"sensor_id\": \"T3\", \"value\": 27.0}, {\"name\": \"T4\", \"reading\": 28.0}]'\n",
    "\n",
    "csv_adapter = CSVAdapter(csv_data)\n",
    "json_adapter = JSONAdapter(json_data)\n",
    "\n",
    "# Process both sources\n",
    "all_data = []\n",
    "for adapter in [csv_adapter, json_adapter]:\n",
    "    raw = adapter.read()\n",
    "    normalized = adapter.to_standard_format(raw)\n",
    "    all_data.extend(normalized)\n",
    "\n",
    "print(\"Integrated data from multiple sources:\")\n",
    "for item in all_data:\n",
    "    print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77dcba",
   "metadata": {},
   "source": [
    "### Data Merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6e1f5",
   "metadata": {},
   "source": [
    "**Figure 10.2: Merging Data Sources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMerger:\n",
    "    \"\"\"Merge data from multiple sources by key.\"\"\"\n",
    "    \n",
    "    def __init__(self, key_field='sensor_id'):\n",
    "        self.key_field = key_field\n",
    "    \n",
    "    def merge(self, *sources):\n",
    "        \"\"\"Merge multiple data sources.\"\"\"\n",
    "        merged = {}\n",
    "        \n",
    "        for source in sources:\n",
    "            for record in source:\n",
    "                key = record.get(self.key_field)\n",
    "                if key not in merged:\n",
    "                    merged[key] = {}\n",
    "                merged[key].update(record)\n",
    "        \n",
    "        return list(merged.values())\n",
    "\n",
    "# Data from different sources\n",
    "readings_source = [\n",
    "    {'sensor_id': 'T1', 'value': 25.5, 'timestamp': '10:00'},\n",
    "    {'sensor_id': 'T2', 'value': 26.0, 'timestamp': '10:00'},\n",
    "]\n",
    "\n",
    "metadata_source = [\n",
    "    {'sensor_id': 'T1', 'location': 'Room A', 'unit': 'celsius'},\n",
    "    {'sensor_id': 'T2', 'location': 'Room B', 'unit': 'celsius'},\n",
    "]\n",
    "\n",
    "calibration_source = [\n",
    "    {'sensor_id': 'T1', 'offset': 0.1, 'last_calibration': '2025-01-01'},\n",
    "    {'sensor_id': 'T2', 'offset': -0.2, 'last_calibration': '2025-01-01'},\n",
    "]\n",
    "\n",
    "# Merge all sources\n",
    "merger = DataMerger()\n",
    "combined = merger.merge(readings_source, metadata_source, calibration_source)\n",
    "\n",
    "print(\"Merged sensor data:\")\n",
    "for record in combined:\n",
    "    print(f\"  {record['sensor_id']}:\")\n",
    "    print(f\"    Value: {record['value']} {record['unit']}\")\n",
    "    print(f\"    Location: {record['location']}\")\n",
    "    print(f\"    Offset: {record['offset']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb41d23",
   "metadata": {},
   "source": [
    "### Data Export Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0024634",
   "metadata": {},
   "source": [
    "**Figure 10.3: Multi-Format Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "class DataExporter:\n",
    "    \"\"\"Export data to multiple formats.\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def to_json(self, indent=2):\n",
    "        return json.dumps(self.data, indent=indent)\n",
    "    \n",
    "    def to_csv(self):\n",
    "        if not self.data:\n",
    "            return \"\"\n",
    "        output = StringIO()\n",
    "        writer = csv.DictWriter(output, fieldnames=self.data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(self.data)\n",
    "        return output.getvalue()\n",
    "    \n",
    "    def to_xml(self, root_tag='data', item_tag='record'):\n",
    "        lines = [f'<{root_tag}>']\n",
    "        for item in self.data:\n",
    "            lines.append(f'  <{item_tag}>')\n",
    "            for key, value in item.items():\n",
    "                lines.append(f'    <{key}>{value}</{key}>')\n",
    "            lines.append(f'  </{item_tag}>')\n",
    "        lines.append(f'</{root_tag}>')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "# Test exporter\n",
    "data = [\n",
    "    {'sensor_id': 'T1', 'value': 25.5, 'unit': 'celsius'},\n",
    "    {'sensor_id': 'T2', 'value': 26.0, 'unit': 'celsius'},\n",
    "]\n",
    "\n",
    "exporter = DataExporter(data)\n",
    "\n",
    "print(\"=== JSON Export ===\")\n",
    "print(exporter.to_json())\n",
    "\n",
    "print(\"\\n=== CSV Export ===\")\n",
    "print(exporter.to_csv())\n",
    "\n",
    "print(\"=== XML Export ===\")\n",
    "print(exporter.to_xml('sensors', 'sensor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294806f4",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Bonus Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9420b",
   "metadata": {},
   "source": [
    "### Exercise B1: Retry Decorator  (Challenge)\n",
    "\n",
    "Create a decorator that retries a function up to N times on failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB1]\n",
    "import functools\n",
    "\n",
    "def retry(max_attempts=3):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_error = None\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_error = e\n",
    "                    print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            raise last_error\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Test\n",
    "counter = 0\n",
    "\n",
    "@retry(max_attempts=3)\n",
    "def unreliable_function():\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter < 3:\n",
    "        raise ConnectionError(\"Temporary failure\")\n",
    "    return \"Success!\"\n",
    "\n",
    "try:\n",
    "    result = unreliable_function()\n",
    "    print(f\"Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"All attempts failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc004f6",
   "metadata": {},
   "source": [
    "### Exercise B2: Data Cache  (Challenge)\n",
    "\n",
    "Implement a cache that stores data with TTL (time-to-live)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2abacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB2]\n",
    "import time\n",
    "\n",
    "class TTLCache:\n",
    "    def __init__(self, ttl_seconds=60):\n",
    "        self.ttl = ttl_seconds\n",
    "        self.cache = {}\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        self.cache[key] = {\n",
    "            'value': value,\n",
    "            'expires': time.time() + self.ttl\n",
    "        }\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        if key not in self.cache:\n",
    "            return default\n",
    "        entry = self.cache[key]\n",
    "        if time.time() > entry['expires']:\n",
    "            del self.cache[key]\n",
    "            return default\n",
    "        return entry['value']\n",
    "    \n",
    "    def clear_expired(self):\n",
    "        now = time.time()\n",
    "        expired = [k for k, v in self.cache.items() if now > v['expires']]\n",
    "        for k in expired:\n",
    "            del self.cache[k]\n",
    "        return len(expired)\n",
    "\n",
    "# Test\n",
    "cache = TTLCache(ttl_seconds=2)\n",
    "cache.set('sensor_1', 25.5)\n",
    "cache.set('sensor_2', 26.0)\n",
    "\n",
    "print(f\"sensor_1: {cache.get('sensor_1')}\")\n",
    "print(f\"sensor_2: {cache.get('sensor_2')}\")\n",
    "print(f\"sensor_3: {cache.get('sensor_3', 'not found')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a751c12",
   "metadata": {},
   "source": [
    "### Exercise B3: Data Stream  (Challenge)\n",
    "\n",
    "Create a DataStream class that simulates real-time sensor data with callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB3]\n",
    "import random\n",
    "import time\n",
    "\n",
    "class DataStream:\n",
    "    def __init__(self, sensor_id):\n",
    "        self.sensor_id = sensor_id\n",
    "        self.callbacks = []\n",
    "        self.running = False\n",
    "    \n",
    "    def subscribe(self, callback):\n",
    "        self.callbacks.append(callback)\n",
    "    \n",
    "    def emit(self, value):\n",
    "        for callback in self.callbacks:\n",
    "            callback(self.sensor_id, value)\n",
    "    \n",
    "    def generate(self, count=5):\n",
    "        \"\"\"Generate simulated readings.\"\"\"\n",
    "        base_value = 25.0\n",
    "        for i in range(count):\n",
    "            value = base_value + random.uniform(-2, 2)\n",
    "            self.emit(round(value, 2))\n",
    "\n",
    "# Test\n",
    "def on_reading(sensor_id, value):\n",
    "    print(f\"Received: {sensor_id} = {value}\")\n",
    "\n",
    "def on_alert(sensor_id, value):\n",
    "    if value > 26:\n",
    "        print(f\"  ‚ö†Ô∏è High temperature alert!\")\n",
    "\n",
    "stream = DataStream('TEMP_001')\n",
    "stream.subscribe(on_reading)\n",
    "stream.subscribe(on_alert)\n",
    "\n",
    "print(\"Generating stream data:\")\n",
    "stream.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e2c53",
   "metadata": {},
   "source": [
    "### Exercise B4: Event Logger  (Challenge)\n",
    "\n",
    "Create an EventLogger that records events with timestamps and supports filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB4]\n",
    "import time\n",
    "\n",
    "class EventLogger:\n",
    "    \"\"\"Event logging system.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "    \n",
    "    def log(self, event_type, message, **data):\n",
    "        self.events.append({\n",
    "            'timestamp': time.time(),\n",
    "            'type': event_type,\n",
    "            'message': message,\n",
    "            **data\n",
    "        })\n",
    "    \n",
    "    def get_by_type(self, event_type):\n",
    "        return [e for e in self.events if e['type'] == event_type]\n",
    "    \n",
    "    def get_recent(self, count=10):\n",
    "        return self.events[-count:]\n",
    "    \n",
    "    def count_by_type(self):\n",
    "        counts = {}\n",
    "        for e in self.events:\n",
    "            counts[e['type']] = counts.get(e['type'], 0) + 1\n",
    "        return counts\n",
    "\n",
    "# Test\n",
    "logger = EventLogger()\n",
    "logger.log('INFO', 'System started')\n",
    "logger.log('DATA', 'Reading received', sensor_id='T1', value=25.5)\n",
    "logger.log('WARNING', 'High temperature', sensor_id='T2', value=35.0)\n",
    "logger.log('DATA', 'Reading received', sensor_id='T1', value=26.0)\n",
    "logger.log('ERROR', 'Connection lost', device='gateway')\n",
    "\n",
    "print(f\"Total events: {len(logger.events)}\")\n",
    "print(f\"Event counts: {logger.count_by_type()}\")\n",
    "print(f\"DATA events: {len(logger.get_by_type('DATA'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f09884",
   "metadata": {},
   "source": [
    "### Exercise B5: Config Validator  (Challenge)\n",
    "\n",
    "Create a configuration validator that checks JSON configs against a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ad161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB5]\n",
    "import json\n",
    "\n",
    "class ConfigValidator:\n",
    "    \"\"\"Validate configuration against schema.\"\"\"\n",
    "    \n",
    "    def __init__(self, schema):\n",
    "        self.schema = schema\n",
    "    \n",
    "    def validate(self, config):\n",
    "        errors = []\n",
    "        self._check(config, self.schema, '', errors)\n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def _check(self, value, schema, path, errors):\n",
    "        # Check type\n",
    "        expected_type = schema.get('type')\n",
    "        if expected_type == 'object' and not isinstance(value, dict):\n",
    "            errors.append(f\"{path}: expected object\")\n",
    "            return\n",
    "        if expected_type == 'string' and not isinstance(value, str):\n",
    "            errors.append(f\"{path}: expected string\")\n",
    "        if expected_type == 'number' and not isinstance(value, (int, float)):\n",
    "            errors.append(f\"{path}: expected number\")\n",
    "        \n",
    "        # Check nested properties\n",
    "        if 'properties' in schema and isinstance(value, dict):\n",
    "            for key, prop_schema in schema['properties'].items():\n",
    "                if key in value:\n",
    "                    self._check(value[key], prop_schema, f\"{path}.{key}\", errors)\n",
    "                elif prop_schema.get('required'):\n",
    "                    errors.append(f\"{path}.{key}: required\")\n",
    "\n",
    "# Test\n",
    "schema = {\n",
    "    'type': 'object',\n",
    "    'properties': {\n",
    "        'name': {'type': 'string', 'required': True},\n",
    "        'port': {'type': 'number', 'required': True},\n",
    "        'debug': {'type': 'boolean'}\n",
    "    }\n",
    "}\n",
    "\n",
    "validator = ConfigValidator(schema)\n",
    "\n",
    "configs = [\n",
    "    {'name': 'sensor_api', 'port': 8080},\n",
    "    {'name': 123, 'port': 8080},\n",
    "    {'port': 8080},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    valid, errors = validator.validate(config)\n",
    "    print(f\"{config}: {'‚úì' if valid else '‚úó ' + str(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96051c02",
   "metadata": {},
   "source": [
    "### Exercise B6: Data Buffer  (Challenge)\n",
    "\n",
    "Implement a circular buffer for sensor data with overflow handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494743d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB6]\n",
    "class CircularBuffer:\n",
    "    \"\"\"Fixed-size circular buffer.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = [None] * capacity\n",
    "        self.head = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def push(self, item):\n",
    "        self.buffer[self.head] = item\n",
    "        self.head = (self.head + 1) % self.capacity\n",
    "        if self.count < self.capacity:\n",
    "            self.count += 1\n",
    "    \n",
    "    def get_all(self):\n",
    "        if self.count < self.capacity:\n",
    "            return self.buffer[:self.count]\n",
    "        # Return in order from oldest to newest\n",
    "        start = self.head\n",
    "        return self.buffer[start:] + self.buffer[:start]\n",
    "    \n",
    "    def is_full(self):\n",
    "        return self.count == self.capacity\n",
    "\n",
    "# Test\n",
    "buffer = CircularBuffer(5)\n",
    "\n",
    "for i in range(8):\n",
    "    buffer.push(f\"Reading_{i}\")\n",
    "    print(f\"After push {i}: {buffer.get_all()}\")\n",
    "\n",
    "print(f\"\\nBuffer full: {buffer.is_full()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de533b",
   "metadata": {},
   "source": [
    "### Exercise B7: Data Compressor  (Challenge)\n",
    "\n",
    "Create a simple run-length encoding compressor for repeated sensor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97804669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB7]\n",
    "class RLECompressor:\n",
    "    \"\"\"Run-length encoding for data compression.\"\"\"\n",
    "    \n",
    "    def compress(self, data):\n",
    "        if not data:\n",
    "            return []\n",
    "        \n",
    "        compressed = []\n",
    "        current = data[0]\n",
    "        count = 1\n",
    "        \n",
    "        for item in data[1:]:\n",
    "            if item == current:\n",
    "                count += 1\n",
    "            else:\n",
    "                compressed.append((current, count))\n",
    "                current = item\n",
    "                count = 1\n",
    "        \n",
    "        compressed.append((current, count))\n",
    "        return compressed\n",
    "    \n",
    "    def decompress(self, compressed):\n",
    "        result = []\n",
    "        for value, count in compressed:\n",
    "            result.extend([value] * count)\n",
    "        return result\n",
    "\n",
    "# Test\n",
    "compressor = RLECompressor()\n",
    "\n",
    "# Sensor data with repeated values\n",
    "data = [25, 25, 25, 26, 26, 27, 27, 27, 27, 25]\n",
    "\n",
    "compressed = compressor.compress(data)\n",
    "decompressed = compressor.decompress(compressed)\n",
    "\n",
    "print(f\"Original: {data}\")\n",
    "print(f\"Compressed: {compressed}\")\n",
    "print(f\"Decompressed: {decompressed}\")\n",
    "print(f\"Size reduction: {len(data)} -> {len(compressed)} tuples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b5edd",
   "metadata": {},
   "source": [
    "### Exercise B8: Query Builder  (Challenge)\n",
    "\n",
    "Create a fluent query builder for filtering sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXB8]\n",
    "class QueryBuilder:\n",
    "    \"\"\"Fluent query builder for data filtering.\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.filters = []\n",
    "    \n",
    "    def where(self, field, op, value):\n",
    "        def make_filter(f, o, v):\n",
    "            if o == '==':\n",
    "                return lambda r: r.get(f) == v\n",
    "            elif o == '!=':\n",
    "                return lambda r: r.get(f) != v\n",
    "            elif o == '>':\n",
    "                return lambda r: r.get(f, 0) > v\n",
    "            elif o == '<':\n",
    "                return lambda r: r.get(f, 0) < v\n",
    "            elif o == 'in':\n",
    "                return lambda r: r.get(f) in v\n",
    "            return lambda r: True\n",
    "        \n",
    "        self.filters.append(make_filter(field, op, value))\n",
    "        return self\n",
    "    \n",
    "    def execute(self):\n",
    "        result = self.data\n",
    "        for f in self.filters:\n",
    "            result = [r for r in result if f(r)]\n",
    "        return result\n",
    "    \n",
    "    def first(self):\n",
    "        results = self.execute()\n",
    "        return results[0] if results else None\n",
    "\n",
    "# Test\n",
    "data = [\n",
    "    {'sensor_id': 'T1', 'value': 25.5, 'location': 'A'},\n",
    "    {'sensor_id': 'T2', 'value': 35.0, 'location': 'A'},\n",
    "    {'sensor_id': 'T3', 'value': 22.0, 'location': 'B'},\n",
    "    {'sensor_id': 'T4', 'value': 28.0, 'location': 'A'},\n",
    "]\n",
    "\n",
    "# Query with method chaining\n",
    "results = (QueryBuilder(data)\n",
    "    .where('location', '==', 'A')\n",
    "    .where('value', '>', 26)\n",
    "    .execute())\n",
    "\n",
    "print(\"Sensors in location A with value > 26:\")\n",
    "for r in results:\n",
    "    print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c3bc0",
   "metadata": {},
   "source": [
    "### Exercise üåâ: Bridge Exercise: Sneak Peek at Week 14  (Preview)\n",
    "\n",
    "**Next week: Final Project!** You've learned every piece of the puzzle ‚Äî variables, loops, functions, files, error handling, OOP, inheritance, modules, and data formats. Now it's time to combine them ALL into one cohesive engineering project.\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Week  1-4:  Python fundamentals (variables, control flow, functions)\n",
    "Week  5-7:  Data structures (lists, dicts, strings)\n",
    "Week  8:    File I/O ‚Äî read/write data\n",
    "Week  9:    Error handling ‚Äî survive bad data\n",
    "Week 10:    OOP ‚Äî organize with classes\n",
    "Week 11:    Inheritance ‚Äî specialize with subclasses\n",
    "Week 12:    Modules ‚Äî structure into packages\n",
    "Week 13:    External data ‚Äî JSON configs, CSV reports\n",
    "\n",
    "Week 14:    üéØ FINAL PROJECT ‚Äî combine everything!\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>üí° Hints</summary>\n",
    "\n",
    "- Your final project should demonstrate skills from at least 5 different weeks\n",
    "- Think about a real-world engineering problem you'd like to solve\n",
    "- The Sensor Data Logger case study is a great template for structuring your project\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cf5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è [EXBridge]\n",
    "# Bridge Exercise: Final Project Planning\n",
    "# Review everything you've learned!\n",
    "\n",
    "skills = {\n",
    "    \"Weeks 1-4\": [\"Variables\", \"Conditionals\", \"Loops\", \"Functions\"],\n",
    "    \"Weeks 5-7\": [\"Lists/Tuples\", \"Dictionaries/Sets\", \"String Methods\"],\n",
    "    \"Week 8\":    [\"File I/O\", \"CSV/JSON reading\", \"Report generation\"],\n",
    "    \"Week 9\":    [\"try/except\", \"Custom exceptions\", \"Validation\"],\n",
    "    \"Week 10\":   [\"Classes\", \"Properties\", \"Composition\"],\n",
    "    \"Week 11\":   [\"Inheritance\", \"Polymorphism\", \"ABC\"],\n",
    "    \"Week 12\":   [\"Modules\", \"Packages\", \"Virtual environments\"],\n",
    "    \"Week 13\":   [\"JSON configs\", \"CSV export\", \"Data pipelines\"],\n",
    "}\n",
    "\n",
    "print(\"üéì YOUR PYTHON SKILL TREE\\n\")\n",
    "total = 0\n",
    "for week, topics in skills.items():\n",
    "    total += len(topics)\n",
    "    print(f\"  {week}: {', '.join(topics)}\")\n",
    "\n",
    "print(f\"\\n  Total skills mastered: {total} üèÜ\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"  üéØ FINAL PROJECT IDEAS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "ideas = [\n",
    "    \"üå°Ô∏è Weather Station Monitor ‚Äî read sensors, log data, generate alerts\",\n",
    "    \"üè≠ Factory Production Tracker ‚Äî CSV input, OOP models, PDF reports\",\n",
    "    \"üìä Student Grade Manager ‚Äî JSON config, data validation, statistics\",\n",
    "    \"ü§ñ Robot Arm Controller ‚Äî command parsing, state machine, error handling\",\n",
    "    \"‚ö° Energy Usage Analyzer ‚Äî CSV data, time analysis, trend detection\",\n",
    "]\n",
    "\n",
    "for i, idea in enumerate(ideas, 1):\n",
    "    print(f\"  {i}. {idea}\")\n",
    "\n",
    "print(f\"\\nüí° Pick a project that excites you!\")\n",
    "print(\"   Next week: build it from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7b9d4",
   "metadata": {},
   "source": [
    "> üí° **Note:** You've mastered every building block of Python programming! Next week's **final project** is your chance to combine everything into one impressive engineering application. Start thinking about what you'd like to build!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bbf37",
   "metadata": {},
   "source": [
    "---\n",
    "## üî¨ Case Study: Sensor Data Logger (Part 6 of 6 ‚Äî Final!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc8d83",
   "metadata": {},
   "source": [
    "The grand finale! We combine **all previous parts** into a production-ready system: JSON configuration, CSV data ingestion, validated processing through our OOP hierarchy, and formatted report output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1ae53",
   "metadata": {},
   "source": [
    "**Goal:** Build the complete end-to-end pipeline: load sensor configuration from JSON, read measurement data from CSV, process through validated sensor classes, and generate a comprehensive report.\n",
    "\n",
    "**What's new:** JSON config loading, CSV data parsing with `csv.DictReader`, complete integration of all 13 weeks of concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be283976",
   "metadata": {},
   "source": [
    "**Case Study 6 ‚Äî Sensor Data Logger: Complete System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22296f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CASE STUDY Part 6: Complete Sensor Data Logger ===\n",
    "# Integrating ALL concepts from Weeks 8-13!\n",
    "import json\n",
    "import csv\n",
    "from io import StringIO\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# --- exceptions.py (Week 9) ---\n",
    "class SensorError(Exception):\n",
    "    pass\n",
    "class InvalidReadingError(SensorError):\n",
    "    def __init__(self, sid, val, rng):\n",
    "        super().__init__(f\"{sid}: {val} outside {rng}\")\n",
    "\n",
    "# --- base.py (Week 10) ---\n",
    "class Sensor(ABC):\n",
    "    def __init__(self, sensor_id, unit, valid_range):\n",
    "        self.sensor_id = sensor_id\n",
    "        self.unit = unit\n",
    "        self.valid_range = valid_range\n",
    "        self._readings = []\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def sensor_type(self): pass\n",
    "    \n",
    "    def validate(self, value):\n",
    "        lo, hi = self.valid_range\n",
    "        if not (lo <= value <= hi):\n",
    "            raise InvalidReadingError(self.sensor_id, value, self.valid_range)\n",
    "    \n",
    "    def add_reading(self, ts, value):\n",
    "        self.validate(value)\n",
    "        self._readings.append((ts, value))\n",
    "    \n",
    "    def get_stats(self):\n",
    "        if not self._readings:\n",
    "            return {\"count\": 0}\n",
    "        vals = [v for _, v in self._readings]\n",
    "        return {\"min\": min(vals), \"max\": max(vals),\n",
    "                \"avg\": sum(vals)/len(vals), \"count\": len(vals)}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.sensor_type}({self.sensor_id})\"\n",
    "\n",
    "# --- sensors.py (Week 11) ---\n",
    "class TemperatureSensor(Sensor):\n",
    "    def __init__(self, sid):\n",
    "        super().__init__(sid, \"¬∞C\", (-40, 80))\n",
    "    @property\n",
    "    def sensor_type(self): return \"Temperature\"\n",
    "\n",
    "class HumiditySensor(Sensor):\n",
    "    def __init__(self, sid):\n",
    "        super().__init__(sid, \"%\", (0, 100))\n",
    "    @property\n",
    "    def sensor_type(self): return \"Humidity\"\n",
    "\n",
    "class PressureSensor(Sensor):\n",
    "    def __init__(self, sid):\n",
    "        super().__init__(sid, \"hPa\", (300, 1100))\n",
    "    @property\n",
    "    def sensor_type(self): return \"Pressure\"\n",
    "\n",
    "# --- utils.py (Week 12) ---\n",
    "SENSOR_MAP = {\"temperature\": TemperatureSensor,\n",
    "              \"humidity\": HumiditySensor,\n",
    "              \"pressure\": PressureSensor}\n",
    "\n",
    "def create_sensor(cfg):\n",
    "    cls = SENSOR_MAP.get(cfg[\"type\"].lower())\n",
    "    if not cls:\n",
    "        raise SensorError(f\"Unknown type: {cfg['type']}\")\n",
    "    return cls(cfg[\"id\"])\n",
    "\n",
    "# === STEP 1: Load JSON Config (Week 13 NEW!) ===\n",
    "config_json = '''{\n",
    "    \"project\": \"Lab Station Alpha\",\n",
    "    \"version\": \"2.0\",\n",
    "    \"sensors\": [\n",
    "        {\"id\": \"T01\", \"type\": \"temperature\", \"location\": \"Room A\"},\n",
    "        {\"id\": \"T02\", \"type\": \"temperature\", \"location\": \"Room B\"},\n",
    "        {\"id\": \"H01\", \"type\": \"humidity\", \"location\": \"Room A\"},\n",
    "        {\"id\": \"P01\", \"type\": \"pressure\", \"location\": \"Roof\"}\n",
    "    ],\n",
    "    \"alert_thresholds\": {\n",
    "        \"temperature\": {\"warn\": 30, \"critical\": 40},\n",
    "        \"humidity\": {\"warn\": 70, \"critical\": 90}\n",
    "    }\n",
    "}'''\n",
    "\n",
    "config = json.loads(config_json)\n",
    "print(f\"üìã Loaded config: {config['project']} v{config['version']}\")\n",
    "\n",
    "# Create sensors from config\n",
    "sensors = {}\n",
    "for cfg in config[\"sensors\"]:\n",
    "    s = create_sensor(cfg)\n",
    "    sensors[s.sensor_id] = s\n",
    "    print(f\"  üì° {s} @ {cfg['location']}\")\n",
    "\n",
    "# === STEP 2: Read CSV Data (Week 13 NEW!) ===\n",
    "csv_data = \"\"\"timestamp,sensor_id,value\n",
    "2024-03-15 08:00,T01,22.5\n",
    "2024-03-15 08:00,T02,24.1\n",
    "2024-03-15 08:00,H01,45.2\n",
    "2024-03-15 08:00,P01,1013.2\n",
    "2024-03-15 08:05,T01,23.1\n",
    "2024-03-15 08:05,T02,N/A\n",
    "2024-03-15 08:05,H01,46.8\n",
    "2024-03-15 08:05,P01,1013.0\n",
    "2024-03-15 08:10,T01,22.8\n",
    "2024-03-15 08:10,T02,35.2\n",
    "2024-03-15 08:10,H01,110.5\n",
    "2024-03-15 08:10,P01,1012.8\"\"\"\n",
    "\n",
    "# === STEP 3: Process with Error Handling (Week 9) ===\n",
    "print(f\"\\nüìù Processing CSV data...\\n\")\n",
    "errors = []\n",
    "reader = csv.DictReader(StringIO(csv_data))\n",
    "for row in reader:\n",
    "    sid = row[\"sensor_id\"]\n",
    "    ts = row[\"timestamp\"]\n",
    "    val_str = row[\"value\"]\n",
    "    try:\n",
    "        sensor = sensors[sid]\n",
    "        sensor.add_reading(ts, float(val_str))\n",
    "        print(f\"  ‚úÖ {sid} @ {ts}: {val_str}\")\n",
    "    except KeyError:\n",
    "        errors.append(f\"Unknown sensor: {sid}\")\n",
    "    except ValueError:\n",
    "        errors.append(f\"{sid} @ {ts}: invalid value '{val_str}'\")\n",
    "        print(f\"  ‚ö†Ô∏è {sid} @ {ts}: SKIP ‚Äî invalid '{val_str}'\")\n",
    "    except InvalidReadingError as e:\n",
    "        errors.append(str(e))\n",
    "        print(f\"  ‚ö†Ô∏è {sid} @ {ts}: SKIP ‚Äî {e}\")\n",
    "\n",
    "# === STEP 4: Generate Report ===\n",
    "thresholds = config.get(\"alert_thresholds\", {})\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"  üìä {config['project']} ‚Äî SENSOR REPORT\")\n",
    "print(f\"{'='*55}\")\n",
    "\n",
    "for sid, sensor in sensors.items():\n",
    "    stats = sensor.get_stats()\n",
    "    loc = next((c[\"location\"] for c in config[\"sensors\"]\n",
    "                if c[\"id\"] == sid), \"?\")\n",
    "    print(f\"\\n  [{sid}] {sensor.sensor_type} @ {loc}\")\n",
    "    if stats[\"count\"] > 0:\n",
    "        print(f\"    Range: {stats['min']:.1f} ‚Äì {stats['max']:.1f} {sensor.unit}\")\n",
    "        print(f\"    Average: {stats['avg']:.2f} {sensor.unit}\")\n",
    "        print(f\"    Readings: {stats['count']}\")\n",
    "        # Check alerts\n",
    "        t_cfg = thresholds.get(sensor.sensor_type.lower(), {})\n",
    "        if t_cfg and stats[\"max\"] >= t_cfg.get(\"critical\", float(\"inf\")):\n",
    "            print(f\"    üö® CRITICAL: max exceeds {t_cfg['critical']}\")\n",
    "        elif t_cfg and stats[\"max\"] >= t_cfg.get(\"warn\", float(\"inf\")):\n",
    "            print(f\"    ‚ö†Ô∏è WARNING: max exceeds {t_cfg['warn']}\")\n",
    "    else:\n",
    "        print(f\"    No valid readings\")\n",
    "\n",
    "# === STEP 5: CSV Export ===\n",
    "print(f\"\\n{'='*55}\")\n",
    "csv_output = StringIO()\n",
    "writer = csv.writer(csv_output)\n",
    "writer.writerow([\"sensor_id\", \"type\", \"location\", \"min\", \"max\",\n",
    "                 \"avg\", \"count\", \"unit\", \"status\"])\n",
    "for sid, sensor in sensors.items():\n",
    "    stats = sensor.get_stats()\n",
    "    loc = next((c[\"location\"] for c in config[\"sensors\"]\n",
    "                if c[\"id\"] == sid), \"?\")\n",
    "    status = \"OK\"\n",
    "    t_cfg = thresholds.get(sensor.sensor_type.lower(), {})\n",
    "    if stats[\"count\"] > 0:\n",
    "        if t_cfg and stats[\"max\"] >= t_cfg.get(\"critical\", float(\"inf\")):\n",
    "            status = \"CRITICAL\"\n",
    "        elif t_cfg and stats[\"max\"] >= t_cfg.get(\"warn\", float(\"inf\")):\n",
    "            status = \"WARNING\"\n",
    "        writer.writerow([sid, sensor.sensor_type, loc,\n",
    "                         f\"{stats['min']:.1f}\", f\"{stats['max']:.1f}\",\n",
    "                         f\"{stats['avg']:.2f}\", stats[\"count\"],\n",
    "                         sensor.unit, status])\n",
    "\n",
    "print(\"  üì§ CSV Export Preview:\")\n",
    "for line in csv_output.getvalue().strip().split(\"\\n\"):\n",
    "    print(f\"    {line}\")\n",
    "\n",
    "# Final summary\n",
    "total = sum(s.get_stats()[\"count\"] for s in sensors.values())\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(f\"  ‚úÖ Processed: {total} valid readings\")\n",
    "print(f\"  ‚ö†Ô∏è Errors: {len(errors)}\")\n",
    "print(f\"  üìä Success rate: {total/(total+len(errors))*100:.0f}%\")\n",
    "print(f\"{'='*55}\")\n",
    "print(\"\\nüéâ CASE STUDY COMPLETE!\")\n",
    "print(\"   Weeks 8‚Üí13: File I/O ‚Üí Errors ‚Üí OOP ‚Üí Inheritance ‚Üí Modules ‚Üí Data\")\n",
    "print(\"   You've built a production-ready sensor monitoring system!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88124562",
   "metadata": {},
   "source": [
    "> üí° **Note:** **üéâ Case Study Complete!** Over 6 weeks, you've built a full sensor monitoring system using every concept from CP2. Your **final project** next week should demonstrate this same level of integration ‚Äî combining multiple concepts into one cohesive application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee083ced",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÆ Submit Your Work\n",
    "\n",
    "**When you're done with all exercises:**\n",
    "1. **Run all exercise cells** (make sure each one executed)\n",
    "2. Fill in your info in the cell below and run it\n",
    "3. Run the next cell to submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c24ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "# üìÆ STEP 1: Fill in your info below, then run this cell\n",
    "#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "STUDENT_ID    = \"\"     # e.g. \"2024001234\"\n",
    "STUDENT_NAME  = \"\"     # e.g. \"Ahmet Yƒ±lmaz\"\n",
    "STUDENT_EMAIL = \"\"     # e.g. \"ahmet.yilmaz@istun.edu.tr\"\n",
    "CLASS_CODE    = \"\"     # code given in class\n",
    "\n",
    "#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "# Don't change anything below this line\n",
    "#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "import re as _re\n",
    "\n",
    "_errors = []\n",
    "if not _re.match(r\"^\\d{6,12}$\", STUDENT_ID):\n",
    "    _errors.append(\"‚ùå Student ID must be 6-12 digits\")\n",
    "if len(STUDENT_NAME.strip().split()) < 2:\n",
    "    _errors.append(\"‚ùå Enter first and last name\")\n",
    "if not STUDENT_EMAIL.strip().lower().endswith(\"@istun.edu.tr\") or len(STUDENT_EMAIL.strip()) < 16:\n",
    "    _errors.append(\"‚ùå Use your @istun.edu.tr email\")\n",
    "if len(CLASS_CODE.strip()) < 4:\n",
    "    _errors.append(\"‚ùå Invalid class code\")\n",
    "\n",
    "if _errors:\n",
    "    for _e in _errors:\n",
    "        print(_e)\n",
    "    print(\"\\n‚ö†Ô∏è  Fix the errors above and run this cell again.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Info OK ‚Äî {STUDENT_NAME} ({STUDENT_ID})\")\n",
    "    print(f\"   {STUDENT_EMAIL}\")\n",
    "    print(f\"\\nüëâ Now run the NEXT cell to submit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b5426",
   "metadata": {},
   "outputs": [],
   "source": "_ORIGINAL_HASHES = {\n    \"ex1\": \"f471df7aafab001ab21e6e9086fdf75c\",\n    \"ex2\": \"ddeccb57e5a67fbf801840ab49f793ac\",\n    \"ex3\": \"125b5cd264aa1346e5f10c3e26b2c9f4\",\n    \"ex4\": \"d2b49b623209b9a5f346400251db1262\",\n    \"ex5\": \"976cd756509fe3b93174bbe2adfe82b9\",\n    \"ex6\": \"8ef3eb89db1d1338d3d9e6258e11d365\",\n    \"ex7\": \"dc09085dadb751190ecabfa199fe7d32\",\n    \"ex8\": \"9c0c07934274c256211a8bab7b2fe598\",\n    \"ex9\": \"dc59b4ff90c72ab124685874797e15d3\",\n    \"ex10\": \"9e3f857b45580e6679ec0e8311ba9c5f\",\n    \"ex11\": \"19682881e51068893954ca9e30241c41\",\n    \"ex12\": \"48bbc5bc2e254eba70dfce6b2532e89a\"\n}\n\n#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n# üìÆ STEP 2: Run this cell to submit\n#‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n# ‚ö†Ô∏è  Make sure you RAN all exercise cells first!\n\nimport json, re, os, urllib.request, hashlib\n\nimport time as _time, datetime as _dt\n\n# ‚îÄ‚îÄ Calculate session duration (heartbeat-based) ‚îÄ‚îÄ\ntry:\n    _active_time = _calc_active_time()\n    _wall_time = int(_time.time() - _SESSION_START)\n    _time_on_page = _active_time\n    _a_min, _a_sec = _active_time // 60, _active_time % 60\n    _w_min, _w_sec = _wall_time // 60, _wall_time % 60\n    print(f\"‚è±Ô∏è  Active time: {_a_min}m {_a_sec}s  (wall: {_w_min}m {_w_sec}s)\")\n    print(f\"üî¢  Cells run: {_CELLS_RUN[0]}  |  Heartbeats: {len(_HEARTBEATS)}\")\n    if _active_time < 120:\n        print(\"‚ö†Ô∏è  Active time < 2 min ‚Äî √ßalƒ±≈ütƒ±rdƒ±ƒüƒ±nƒ±z h√ºcre sayƒ±sƒ± d√º≈ü√ºk olabilir.\")\nexcept NameError:\n    _time_on_page = 0\n    _wall_time = 0\n    print(\"‚ö†Ô∏è  Timer not started ‚Äî run the first cell next time for time tracking.\")\n\n\nWEEK = \"Week_13\"\nURL  = \"https://script.google.com/macros/s/AKfycbyf1D3HGSAX4MoIhNlAuWlGrFyyvbM5MIv7ZsLxrVDlATUihrRGEAaibvIZYlCfd8Me/exec\"\n\n# ‚îÄ‚îÄ Check info was filled in ‚îÄ‚îÄ\ntry:\n    _sid = STUDENT_ID.strip()\n    _sname = STUDENT_NAME.strip()\n    _semail = STUDENT_EMAIL.strip().lower()\n    _scode = CLASS_CODE.strip().upper()\nexcept NameError:\n    raise SystemExit(\"‚ùå Run the cell above first to set your info!\")\n\nif not _sid or not _sname or not _semail or not _scode:\n    raise SystemExit(\"‚ùå Run the cell above first ‚Äî some fields are empty.\")\n\n# ‚îÄ‚îÄ Extract exercise answers from IPython history ‚îÄ‚îÄ\n_answers = {}\ntry:\n    _ipy = get_ipython()\n    _hist = _ipy.history_manager.get_range(output=False)\n    for _sess, _line, _src in _hist:\n        _m = re.match(r\"#\\s*‚úèÔ∏è\\s*\\[EX(\\d+)\\]\", _src)\n        if _m:\n            _ex_id = \"ex\" + _m.group(1)\n            _lines = _src.split(\"\\n\")\n            _clean = \"\\n\".join(_lines[1:]).strip()\n            _answers[_ex_id] = {\n                \"code\": _clean,\n                \"modified\": hashlib.md5(_clean.encode()).hexdigest() != _ORIGINAL_HASHES.get(_ex_id, \"\")\n            }\nexcept Exception:\n    pass\n\n# ‚îÄ‚îÄ Fallback: also check In[] from current session ‚îÄ‚îÄ\nif not _answers:\n    try:\n        for _src in In:\n            if not _src:\n                continue\n            _m = re.match(r\"#\\s*‚úèÔ∏è\\s*\\[EX(\\d+)\\]\", _src)\n            if _m:\n                _ex_id = \"ex\" + _m.group(1)\n                _lines = _src.split(\"\\n\")\n                _clean = \"\\n\".join(_lines[1:]).strip()\n                _answers[_ex_id] = {\n                    \"code\": _clean,\n                    \"modified\": hashlib.md5(_clean.encode()).hexdigest() != _ORIGINAL_HASHES.get(_ex_id, \"\")\n                }\n    except NameError:\n        pass\n\n# ‚îÄ‚îÄ Fallback: try reading notebook file (VS Code) ‚îÄ‚îÄ\nif not _answers:\n    _nb_path = None\n    try:\n        _nb_path = __vsc_ipynb_file__\n    except NameError:\n        _candidates = [f for f in os.listdir(\".\") if f.endswith(\".ipynb\") and WEEK in f]\n        if len(_candidates) == 1:\n            _nb_path = _candidates[0]\n    if _nb_path and os.path.exists(str(_nb_path)):\n        with open(str(_nb_path), \"r\", encoding=\"utf-8\") as _f:\n            _nb = json.load(_f)\n        for _cell in _nb[\"cells\"]:\n            if _cell[\"cell_type\"] != \"code\":\n                continue\n            _src = \"\".join(_cell[\"source\"]) if isinstance(_cell[\"source\"], list) else _cell[\"source\"]\n            _m = re.match(r\"#\\s*‚úèÔ∏è\\s*\\[EX(\\d+)\\]\", _src)\n            if _m:\n                _ex_id = \"ex\" + _m.group(1)\n                _lines = _src.split(\"\\n\")\n                _clean = \"\\n\".join(_lines[1:]).strip()\n                _answers[_ex_id] = {\n                    \"code\": _clean,\n                    \"modified\": hashlib.md5(_clean.encode()).hexdigest() != _ORIGINAL_HASHES.get(_ex_id, \"\")\n                }\n\nprint(f\"üìù Found {len(_answers)} exercise(s): {', '.join(sorted(_answers.keys()))}\")\n\nif not _answers:\n    print(\"\\n‚ö†Ô∏è  No exercise answers found!\")\n    print(\"Make sure you RAN all exercise cells before submitting.\")\n    raise SystemExit()\n\n# ‚îÄ‚îÄ Send ‚îÄ‚îÄ\n_data = json.dumps({\n    \"week\": WEEK,\n    \"studentId\": _sid,\n    \"studentName\": _sname,\n    \"studentEmail\": _semail,\n    \"classCode\": _scode,\n    \"source\": \"cp2-notebook\",\n    \"timeOnPage\": _time_on_page,\n    \"wallTime\": _wall_time if \"_wall_time\" in dir() else 0,\n    \"cellsRun\": _CELLS_RUN[0] if \"_CELLS_RUN\" in dir() else 0,\n    \"sessionStart\": _SESSION_START_STR if \"_SESSION_START_STR\" in dir() else \"\",\n    \"sessionEnd\": _dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") if \"_dt\" in dir() else \"\",\n    \"answers\": _answers\n}).encode(\"utf-8\")\n\nprint(\"üì° Submitting...\")\n\ntry:\n    _req = urllib.request.Request(URL, data=_data, headers={\"Content-Type\": \"text/plain\"}, method=\"POST\")\n    _resp = urllib.request.urlopen(_req, timeout=30)\n    _result = json.loads(_resp.read().decode())\n    if _result.get(\"success\"):\n        print(f\"\\n‚úÖ {_result['message']}\")\n        print(\"üìß Check your email for confirmation.\")\n    else:\n        print(f\"\\n‚ùå {_result.get('message', 'Submission failed')}\")\nexcept Exception as _e:\n    try:\n        _req = urllib.request.Request(URL, data=_data, headers={\"Content-Type\": \"text/plain\"}, method=\"POST\")\n        urllib.request.urlopen(_req, timeout=10)\n    except:\n        pass\n    print(f\"\\n‚ö†Ô∏è  Request sent ‚Äî check your email for confirmation.\")\n    print(f\"(If no email arrives, try again or contact your instructor)\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}